{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Notebook Author:<br>Felix Gonzalez, P.E. <br> Adjunct Instructor, <br> Division of Professional Studies <br> Computer Science and Electrical Engineering <br> University of Maryland Baltimore County <br> fgonzale@umbc.edu\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Acknowledgements:<br>\n",
    "This dataset was generated from The Movie Database API (https://www.kaggle.com/datasets/tmdb/themoviedb.org). This product uses the TMDb API but is not endorsed or certified by TMDb. Their API also provides access to data on many additional movies, actors and actresses, crew members, and TV shows. You can try it for yourself here: https://www.themoviedb.org/documentation/api.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Notebook Goal](#Notebook-Goal)<br>\n",
    "- [Source Data](#Source-Data)<br>\n",
    "- [Library Loading](#Library-Loading)<br>\n",
    "- [Default Jupyter Notebook Settings](#Default-Jupyter-Notebook-Settings)<br>\n",
    "- [Progress Status Function](#Progress-Status-Function)<br>\n",
    "- [Data Loading](#Data-Loading)<br>\n",
    "- [Categorical Columns Menu Option Lists for Filtering](#Categorical-Columns-Menu-Option-Lists-for-Filtering)<br>\n",
    "- [Text Normalization](#Text-Normalization)<br>\n",
    "- [Number Formatting Functions](#Number-Formatting-Functions)<br>\n",
    "- [Dashboard Widgets and Functions](#Dashboard-Widgets-and-Functions)<br>\n",
    "    - [Data Filtering Widget](#Data-Filtering-Widget)<br>\n",
    "    - [Filtered Dataframe: Unique Values Dictionary](#Filtered-Dataframe:-Unique-Values-Dictionary)<br>\n",
    "    - [Statistics Widget Tab](#Statistics-Widget-Tab)<br>\n",
    "    - [Similarity Search Functions](#Similarity-Search-Functions)<br>\n",
    "    - [Data Transformation Functions](#Data-Transformation-Functions)<br>\n",
    "    - [Data Plotting Functions](#Data-Plotting-Functions)<br>\n",
    "    - [Text Analysis and Text Modeling Widget](#Text-Analysis-and-Text-Modeling-Widget)<br>\n",
    "        - [Text Analysis and Text Modeling Widget Functions](#Text-Analysis-and-Text-Modeling-Widget-Functions)<br>\n",
    "        - [Bag of Words Text Model Function](#Bag-of-Words-Text-Model-Function)<br>\n",
    "        - [Dimensionality Reduction Functions](#Dimensionality-Reduction-Functions)<br>\n",
    "        - [PCA Dimensionality Reduction Functions](#PCA-Dimensionality-Reduction-Functions)<br>\n",
    "        - [TSNE Dimensionality Reduction Functions](#TSNE-Dimensionality-Reduction-Functions)<br>\n",
    "        - [Cluster Plots and Plot Projection Function](#Cluster-Plots-and-Plot-Projection-Function)<br>\n",
    "        - [DBSCAN Clustering Functions](#DBSCAN-Clustering-Functions)<br>\n",
    "        - [DBSCAN Clustering (% Records to Cluster) Functions](#DBSCAN-Clustering-(%-Records-to-Cluster)-Functions)<br>\n",
    "        - [DBSCAN Clustering (Max Clusters w/Optimal EPS) Functions](#DBSCAN-Clustering-(Max-Clusters-w/Optimal-EPS)-Functions)<br>\n",
    "        - [DBSCAN Clustering (Most Dense Cluster w/Optimal EPS) Widget and Functions](#DBSCAN-Clustering-(Most-Dense-Cluster-w/Optimal-EPS)-Widget-and-Functions)<br>\n",
    "        - [DBSCAN Clustering (Custom EPS and Min Samples in a Cluster) Functions](#DBSCAN-Clustering-(Custom-EPS-and-Min-Samples-in-a-Cluster)-Functions)<br>\n",
    "        - [KMeans Clustering Functions](#KMeans-Clustering-Functions)<br>\n",
    "    - [Top Terms WordCloud Widget and Clustering Plots Functions](#Top-Terms-WordCloud-Widget-and-Clustering-Plots-Functions)<br>\n",
    "        - [Top Terms Wordcloud Widget](#Top-Terms-Wordcloud-Widget)<br>\n",
    "    - [Predictive Modeling Widget](#Predictive-Modeling-Widget)<br>\n",
    "    - [Reports Widget Functions](#Reports-Widget-Functions)<br>\n",
    "    - [Export Filtered Dataframe Function](#Export-Filtered-Dataframe-Function)<br>\n",
    "    - [Reclustering: Filters the Selected Cluster](#Reclustering:-Filters-the-Selected-Cluster)<br>\n",
    "    - [Main Data Filtering Widget](#Main-Data-Filtering-Widget)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Dashboard Template\n",
    "\n",
    "Python has various libraries that allow for data dashboards. In this notebook, a dashboard is developed using [Jupyter Notebook Widgets](https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20List.html). Python has various other libraries used for developing dashboards. These include Anaconda Panel, Plotly Dash, Streamlit, Voila among others. Amazon Web Services, MS Azure, Google Cloud Platform, Microstrategy, Palantir Foundry, Tableau are other commercially available platforms that have their own platforms, frameworks and libraries that also allow for developing data analysis dashboard like applications.   \n",
    "\n",
    "Jupyter Notebook Widgets have one advantage in that they can be quickly developed in a Jupyter Notebook and allows a data scientist to develop a concept and explore potential capabilities of a dashboard while also allowing to identify early limitations of the data and the tools without spending the resources in developing a webbased dashboard. However, the main disadvantage of Jupyter Notebook Widgets is that they cannot be deployed in a Website framework or an application easily and will need to be moved to another platform if the stakeholder decides to proceed. The other libraries above all have different advanatages and limitations and would need to be evaluated separately depending on the needs of the stakeholders and the organization.\n",
    "\n",
    "References: <br>\n",
    "- https://medium.com/spatial-data-science/the-best-tools-for-dashboarding-in-python-b22975cb4b83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Goal\n",
    "This notebook leverages the output of the 1_Data_Cleaning_Template.ipynb Notebook and continues into the next step to further analyze the data to extract insights. This includes but is not limited to provide visualizations, plots, charts, statistics, correlations, relationship between features, patterns, trends, anomalies, test hypothesis for feature selection, identify potential features to use in clustering or classification and other AI/ML algorithms, etc.\n",
    "\n",
    "The main goal is to show what the EDA and visualizations would show and look for. There are hundreds of visualization types. What will drive which plots you create will be what are you trying to achieve. What story are you trying to make? What is the decision that you want to make? How can you convince your stakeholders? The data will also limit on what you will be able to do and show. The purpose of this notebook is to give you some ideas on plots that we can make and what general functions we can use. There may be other more advanced functions that can be used as well and it is important to review sample plots from visualization libraries like Matplotlib, SNS, Plotly, JSD3 and others. Their example gallery will potentially give you more ideas on things that you can do.\n",
    "\n",
    "This notebook can be used as a template for quickly developing data analysis dashboards of any data as long as it has been cleaned using the 1_Data_Cleaning_Template.ipynb. Once the data is loaded as a dataframe (DF), most functions can be used as is with some modifications. Modifications include updating the options of the filtering (in section named \"Categorical Columns Menu Option Lists for Filtering\" and \"DF_DATA\" dictionary) and the filtering options in the \"Main Data Filtering Widget\" as well as the df_data_filtering and all variables as they relate to the filtering columns names and unique values in the \"Main Data Filtering Widget\" and the \"Data Filtering Widget\". Identifying the other functions that need to be modified is easier just running the widgets and fixing where the errors are at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Data\n",
    "Detailed description of the data can be found at the 1_Data_Cleaning_Template.ipynb Notebook. This Jupyter Notebook uses the outputs from the 1_Data_Cleaning_Template.ipynb Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Loading\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import unique, where\n",
    "from collections import Counter\n",
    "from statistics import mean # Mean function from statistics module\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  # For bag of words.\n",
    "from sklearn.cluster import KMeans, DBSCAN # K-Means and DBSCAN\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances, silhouette_samples, mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA # used for PCA Dimensionality Reduction\n",
    "from sklearn.manifold import TSNE # Used for TSNE Dimensionality Reduction.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LinearRegression # Linear Regression Model.\n",
    "from sklearn.feature_selection import f_regression # Metrics. F-Regression (p-value and f-statistic)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy.stats import poisson # Poisson Distribution\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import nltk # Natural Langage Toolkit\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer # For lemmitization and Stemming\n",
    "from nltk import pos_tag # For parts of speech\n",
    "from nltk import word_tokenize # To create tokens\n",
    "from nltk.corpus import stopwords, wordnet # Stopwords and POS tags\n",
    "#nltk.download #(One time to download 'stopwords')\n",
    "#nltk.download # (One time to download 'punkt')\n",
    "#nltk.download #(One time to download 'averaged_perceptron_tagger')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image, display, HTML, clear_output\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Jupyter Notebook Settings\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    "References: <br>\n",
    "- Color cycling (https://matplotlib.org/stable/gallery/color/named_colors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) # PD has a limit of 50 characters.  This takes out the limit and uses the full text.\n",
    "pd.options.display.float_format = \"{:.4f}\".format # Pandas displays float numbers as 4 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipyWidget Vertical Scroll Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default color cycle for MatPlotLib Plots\n",
    "#plt.rcParams['axes.prop_cycle'] = plt.cycler(color=['b', 'r', 'g']) # Specifies specific color cycling (https://matplotlib.org/stable/gallery/color/named_colors.html)\n",
    "#plt.style.available # Available Color Styles\n",
    "plt.style.use('tableau-colorblind10') # Defining a specific color style to use.  Tableu-Colorblind\n",
    "#plt.style.use('seaborn-colorblind') # Defining a specific color style to use. Seaborn-colorblind\n",
    "\n",
    "#colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] # Extract Colors being defined in the plt.style.use                       \n",
    "#print('\\n'.join(color for color in colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Status Function\n",
    "[Return to Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress Bar Function. Used in loops.\n",
    "def progress_status(step, total_steps):\n",
    "    #Progress Status\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Currently processing step: {step} of {total_steps}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "Refrences: <br>\n",
    "- Encoding https://stackoverflow.com/questions/57061645/why-is-%C3%82-printed-in-front-of-%C2%B1-when-code-is-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING CSV FILE\n",
    "# Na_values may need to be reviewed as some datasets may include an accronym.\n",
    "# For example, 'NA' may be an abbreviation for 'North America'.\n",
    "df_data = pd.read_csv('./output_data/df_data_clean.csv', \n",
    "                      encoding = \"utf-8-sig\",\n",
    "                      parse_dates=['release_date', 'release_cy_quarter', 'release_cy_month'],\n",
    "                      keep_default_na=False,\n",
    "                      na_values=['', '-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A','N/A', '#NA', 'NULL', 'NaN', '-NaN', 'nan', '-nan']) \n",
    "\n",
    "# IF LOADING EXCEL FILE: use pd.read_excel.\n",
    "#df_data = pd.read_excel('.\\input_data\\FILE_NAME.xlsx', parse_dates=['Date', 'Final Date'])\n",
    "\n",
    "# Encoding \"cp1252\" or \"utf-8-sig\" used so that Excel does not create special characters. Standard Python is utf-8.\n",
    "# See reference for explanation https://stackoverflow.com/questions/57061645/why-is-%C3%82-printed-in-front-of-%C2%B1-when-code-is-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Column selection for selecting columsn in loops used in the data cleaning, visualization and model functions below.\n",
    "dfcolumns = list(df_data.columns.values)\n",
    "dfcolumns_index = pd.DataFrame(dfcolumns, columns=['column'])\n",
    "pd.set_option('display.max_rows', None)\n",
    "#dfcolumns_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Columns Menu Option Lists for Filtering\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "This sections creates a list of unique categorical values in each feature or column in order to be able to later as filtering options of dropdown menus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes the type of the data so that the NAN are considered a string.\n",
    "# In cases you may need to changes the NAN to 0 to fix issue with the Stacking. \n",
    "#df_data['feature_name'] = df_data['feature_name'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the \"genres\" column to a string.\n",
    "df_data['genres'] = df_data['genres'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record Date Calendar Years\n",
    "release_cy_list = (df_data['release_cy'].unique()).tolist()\n",
    "release_cy_list.sort()\n",
    "#release_cy_list = ['NaT']+release_cy_list # Includes 'NaT'. Only needed if data has undefined year.\n",
    "release_cy_list = list(dict.fromkeys([element for element in release_cy_list])) # Removes and Duplicates\n",
    "#release_cy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develops a unique list of genres to be able to use it in the menus.\n",
    "\n",
    "# First and Last index of the Movie Genres. \n",
    "# Two methods: First using the columns of the main dataframe, the other of the dfcolumns_index dataframe.\n",
    "first_genre_index = df_data.columns.to_list().index(\"Action\")\n",
    "#first_genre_index = dfcolumns_index.index[dfcolumns_index['column'] == str('Action')][0] \n",
    "\n",
    "last_genre_index = df_data.columns.to_list().index(\"Western\")\n",
    "#last_genre_index = dfcolumns_index.index[dfcolumns_index['column'] == str('Western')][0] \n",
    "\n",
    "# List of Unique Genres.\n",
    "genres_list = sorted(dfcolumns_index['column'][first_genre_index:last_genre_index+1].to_list())\n",
    "#genres_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREND OPTIONS LISTS\n",
    "trends = [3, 5, 10] # Note the start_year needs the trend to be adjusted to -1.\n",
    "\n",
    "# TEXT MODELING AND CLUTERING OPTIONS LIST\n",
    "TEXT_MODEL_list = ['None', 'PLACEHOLDER: Topic Model: LDA', 'Clustering: Kmeans (user defined clusters)', \n",
    "                   'Clustering: Kmeans w/Optimal K',\n",
    "                   'Clustering: DBSCAN (Max Clusters w/Optimal EPS)', \n",
    "                   'Clustering: DBSCAN (Densest Clusters w/Optimal EPS)', \n",
    "                   'Clustering: DBSCAN (% Records to Cluster)',\n",
    "                   'Clustering: DBSCAN (Custom EPS and Min Samples)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "Recall that the text was normalized in the data cleaning notebook which created a norm_text_lemmatized and norm_text_stemmed columns. In order to successfully use text clustering, similarity searching and other NLP tasks that may use the text normalization you will need to ensure that both text normalization functions are the same.\n",
    "\n",
    "References: <br>\n",
    "- Lemmatization: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#wordnetlemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Specific Stopwords. This list is added in post processing as part of input in the data dashboards.\n",
    "# This allows to explore real-time the effect of adding stop words.\n",
    "stopwords_to_add_L2 = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords to add need to be developed by a SME familiar with the corpus.\n",
    "# These stopwords_to_add are added in pre-processing as part of the text normalization function.\n",
    "stopwords_to_add = [''] # If you wanted to add a corpus related stopword list add them here.\n",
    "\n",
    "# NLTK library stopwords\n",
    "stopwords_custom = stopwords.words('english') + [x.lower() for x in stopwords_to_add]\n",
    "\n",
    "# In some cases you want to consider 2-grams especially with the word 'no', 'not', 'nor'.\n",
    "# For example 'no fire'.  Removing the word 'no' from the stopwords list allows this to occur.\n",
    "remove_as_stopword = ['no', 'not', 'nor']\n",
    "stopwords_custom = list(filter(lambda w: w not in remove_as_stopword, stopwords_custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalization(text, word_reduction_method):\n",
    "    text = str(text) # Convert narrative to string.\n",
    "    df = pd.DataFrame({'': [text]}) # Converts narrative to a dataframe format use replace functions.\n",
    "    df[''] = df[''].str.lower() # Covert narrative to lower case.\n",
    "    df[''] = df[''].str.replace(\"\\d+\", \" \", regex = True) # Remove numbers\n",
    "    df[''] = df[''].str.replace(\"[^\\w\\s]\", \" \", regex = True) # Remove special characters\n",
    "    df[''] = df[''].str.replace(\"_\", \" \", regex = True) # Remove underscores characters\n",
    "    df[''] = df[''].str.replace('\\s+', ' ', regex = True) # Replace multiple spaces with single\n",
    "    text = str(df[0:1]) # Extracts narrative from dataframe.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # Tokenizer.\n",
    "    tokens = tokenizer.tokenize(text) # Tokenize words.\n",
    "    filtered_words = [w for w in tokens if len(w) > 1 if not w in stopwords_custom] # Note remove words of 1 letter only. Can increase to higher value as needed.\n",
    "    if word_reduction_method == 'Lemmatization':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        reduced_words=[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in filtered_words] # Lemmatization.  The second argument is the POS tag.\n",
    "    if word_reduction_method == 'Stemming':\n",
    "        stemmer = PorterStemmer() # Stemming also could make the word unreadable but is faster than lemmatization.\n",
    "        reduced_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    return \" \".join(reduced_words) # Join words with space.\n",
    "\n",
    "def get_wordnet_pos(word): # Reference: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#wordnetlemmatizer\n",
    "    #\"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number Formatting Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "The functions in this section allow dynamic changes of the format of numbers (e.g., currency, integer and float numbers). For example, it automatically chagnes the values from 1,000,000 to 1M given the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define the axis label with Dollar Sign and e^x. Args are the value and tick position\n",
    "def currency_number_format(x, pos): \n",
    "    if x >= 1e6:\n",
    "        s = '${:,.2f}M'.format(x*1e-6) # E.g., 1.00M.  If numbers are too big might have to create another level\n",
    "    elif (1e6 > x) & (x >=1e3):\n",
    "        s = '${:,.1f}K'.format(x*1e-3) # E.g., $999K - $1K\n",
    "    elif (1e3 > x) & (x >= 1e2):\n",
    "        s = '${:,.1f}'.format(x) # E.g., $999.99 - $100\n",
    "    elif (1e2 > x) & (x >= 0.01):\n",
    "        s = '${:,.2f}'.format(x) # E.g., $99.99 - $0.01\n",
    "    elif 0.01 > x: # E.g., $0\n",
    "        s = '${:,.0f}'.format(x)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define the axis label and e^x. Args are the value and tick position\n",
    "def number_format(x, pos): \n",
    "    if x >= 1e6:\n",
    "        s = '{:,.2f}M'.format(x*1e-6) # E.g., 1.00M.  If numbers are too big might have to create another level\n",
    "    elif (1e6 > x) & (x >=1e3):\n",
    "        s = '{:,.1f}K'.format(x*1e-3) # E.g., $999K - $1K\n",
    "    elif (1e3 > x) & (x >= 1e2):\n",
    "        s = '{:,.1f}'.format(x) # E.g., $999.99 - $100\n",
    "    elif (1e2 > x) & (x >= 0.01):\n",
    "        s = '{:,.2f}'.format(x) # E.g., $99.99 - $0.01\n",
    "    elif 0.01 > x: # E.g., $0\n",
    "        s = '{:,.0f}'.format(x)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define the axis label with and e^x. Args are the value and tick position\n",
    "def int_number_format(x, pos): \n",
    "    if x >= 1e6:\n",
    "        s = '{:,.3f}M'.format(x*1e-6) # E.g., 1.00M.  If numbers are too big might have to create another level\n",
    "    elif (1e6 > x) & (x >=1e4):\n",
    "        s = '{:,.1f}K'.format(x*1e-3) # E.g., 100K - 9,999\n",
    "    elif (1e4 > x) & (x >=1e3):\n",
    "        s = '{:,.0f}'.format(x) # E.g., 9,999 - 1000\n",
    "    elif (1e3 > x) & (x >= 0):\n",
    "        s = '{:,.0f}'.format(x) # E.g., 999 - 100\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard Widgets and Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "References: <br>\n",
    "- List of widgets:https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20List.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Filtering Widget\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "The Data Filtering Widget takes as input all the filters of the Main Data Filtering Widget and then calls the objects and sub-widgets in each of the tabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRIMARY WIDGET DATA FUNCTION\n",
    "def filter_data(release_cy_filter, mult_genres_filter, genres_filter, \n",
    "                SIM_SEARCH_filter, SIM_SEARCH_THRESHOLD_filter, SIM_SEARCH_WORD_PROCESS_filter, \n",
    "                SIM_SEARCH_VECT_filter, STOPWORDS_filter, PRINT_filter):\n",
    "    \n",
    "    global df_data_filtered, print_winput, reclustering_counter, release_cy_winput, genres_winput, STOPWORDS_winput\n",
    "    \n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    # Redefining the input filter variables as a new name to pass it to other functions thru a global variable. \n",
    "    release_cy_winput = release_cy_filter\n",
    "    genres_winput = genres_filter\n",
    "    mult_genres_filter = mult_genres_filter\n",
    "    STOPWORDS_winput = STOPWORDS_filter.split(',') # Note that stopwords added here are later passed thru the text normalization.\n",
    "    print_winput = PRINT_filter\n",
    "    \n",
    "    # Filtering Data\n",
    "    if mult_genres_filter == 'AND':\n",
    "        df_data_filtered = df_data.loc[(df_data['release_cy'] >= min(release_cy_winput)) &\n",
    "                                       (df_data['release_cy'] <= max(release_cy_winput)) &\n",
    "                                       (df_data['genres'].str.contains(r'^(?=.*\\b' + r'\\b)(?=.*\\b'.join(genres_winput) \n",
    "                                                                       + r'\\b).*$', flags=re.IGNORECASE, regex=True))\n",
    "                                      ].copy().reset_index(drop = True)\n",
    "        \n",
    "    if mult_genres_filter == 'OR':\n",
    "        df_data_filtered = df_data.loc[(df_data['release_cy'] >= min(release_cy_winput)) &\n",
    "                                       (df_data['release_cy'] <= max(release_cy_winput)) &\n",
    "                                       (df_data['genres'].str.contains('|'.join(genres_winput), \n",
    "                                                                       flags=re.IGNORECASE, regex=True))\n",
    "                                      ].copy().reset_index(drop = True)\n",
    "    \n",
    "    if len(SIM_SEARCH_filter) > 0: # If there is text in the Input Box runs this function to further filter by the Similarity Search parameters\n",
    "        bow_vectorizer_data(input_data = df_data_filtered,\n",
    "                            vect = SIM_SEARCH_VECT_filter,\n",
    "                            word_preprocessing = SIM_SEARCH_WORD_PROCESS_filter, \n",
    "                            max_df = 1.0, min_df = 0.0001)\n",
    "        input_bow_rank_df(text = SIM_SEARCH_filter, \n",
    "                          word_reduction_method = SIM_SEARCH_WORD_PROCESS_filter, \n",
    "                          threshold = SIM_SEARCH_THRESHOLD_filter)\n",
    "        \n",
    "    # Dynamic Sub-Widget Dictionary of text description, feature and unique cateogories list.\n",
    "    df_unique_values_dict()\n",
    "    \n",
    "    # TABS\n",
    "    statistics_tab = widgets.Output()\n",
    "    predictive_tab = widgets.Output()\n",
    "    correlation_tab = widgets.Output()\n",
    "    auto_trend_tab = widgets.Output()\n",
    "    feature_plots_tab = widgets.Output()\n",
    "    top_terms_tab = widgets.Output()\n",
    "    text_modelling_tab = widgets.Output()\n",
    "    text_plot_tab = widgets.Output()\n",
    "    similarity_stats_tab = widgets.Output()\n",
    "    networkx_tab = widgets.Output()\n",
    "    summarization_tab = widgets.Output()\n",
    "    filtered_reports_tab = widgets.Output()\n",
    "    recluster_tab = widgets.Output() \n",
    "    \n",
    "    # TABS PROPERTIES\n",
    "    tab = widgets.Tab(children = [statistics_tab, predictive_tab, correlation_tab, auto_trend_tab, feature_plots_tab, \n",
    "                                  top_terms_tab, text_modelling_tab, text_plot_tab, similarity_stats_tab, networkx_tab,  \n",
    "                                  summarization_tab, filtered_reports_tab, recluster_tab])\n",
    "    tab.set_title(0, 'Stats')\n",
    "    tab.set_title(1, 'Predictive')\n",
    "    tab.set_title(2, 'Correlation')\n",
    "    tab.set_title(3, 'AutoTrends')\n",
    "    tab.set_title(4, 'Ftr. Plots')\n",
    "    tab.set_title(5, 'Top Terms')\n",
    "    tab.set_title(6, 'Txt Model')\n",
    "    tab.set_title(7, 'Txt Plot')\n",
    "    tab.set_title(8, 'Sim. Stats')\n",
    "    tab.set_title(9, 'Network')\n",
    "    tab.set_title(10, 'Summary')\n",
    "    tab.set_title(11, 'Reports')\n",
    "    tab.set_title(12, 'Recluster')\n",
    "\n",
    "    display(tab)\n",
    "    \n",
    "    # Resets reclustering counter if the Main widget runs.\n",
    "    reclustering_counter = 0\n",
    "    \n",
    "    # TABS INFORMATION AND WIDGET AND FUNCTION CALLING\n",
    "    with statistics_tab:\n",
    "        display(HTML(f'<h3>Data Statistics and Plots<h3>'))\n",
    "        stats_title_and_records_widget()\n",
    "\n",
    "    with predictive_tab:\n",
    "        print('The predictive analysis aims to leverage predictive and statistical models such as Regressions (e.g., Linear Regression, Multiple Linear Regression), Poisson Distribution, Monte Carlo Simulations, and other models to provide insights on future direction of records.')\n",
    "        predictive_widget()\n",
    "        \n",
    "    with correlation_tab:\n",
    "        print('The Correlation Tab can allow the exploration of the correlation betweem the features in the data using a correlation heatmap and different correlation coeficients (e.g., Pearson, Spearman, Chi-Square, Cramers V).')\n",
    "        print('https://towardsdatascience.com/statistics-in-python-using-chi-square-for-feature-selection-d44f467ca745')\n",
    "        print('https://www.kaggle.com/chrisbss1/cramer-s-v-correlation-matrix')\n",
    "        print('https://link.medium.com/Y7ZKRp1LEnb')\n",
    "        print('https://datagy.io/python-correlation-matrix/')\n",
    "        \n",
    "    with auto_trend_tab:\n",
    "        print('The Automatic Trending aims to leverage automating the calculation of the slope of each of the features (e.g., categories, etc.) and provide results of the highest increasing trends.')\n",
    "        \n",
    "    with feature_plots_tab:\n",
    "        print('Feature Plots tab will allow selecting and plotting any two features to evaluate relationship.')\n",
    "    \n",
    "    with top_terms_tab:        \n",
    "        display(HTML(f'<h3>Top Terms within Corpus<h3>'))\n",
    "        print('The Top Terms provide a list of the top terms given the parameters used.')\n",
    "        top_terms_widget()\n",
    "    \n",
    "    with text_modelling_tab:\n",
    "        display(HTML(f'<h3>Text Analysis and Modeling<h3>'))\n",
    "        print('The text analysis and modeling algorithms perform several types of analyses. This include Latent Derechlit Allocation (LDA) which is a type of topic model as well as text clustering using Kmeans and DBSCAN algorithms. These algorithms provide insights by clustering reports into similar groups of reports and givin statistics on the top terms. The non-zero cluster labels in this dashboard are sorted by the mean of the top 10 terms scores. For example, Cluster #1 will have a higher average score than Cluster #2 and so on.')\n",
    "        text_modelling_widget()\n",
    "\n",
    "    with text_plot_tab:\n",
    "        print('Teh Text Plot tab will use Scatter Text Library to visualize text and provide insights.')\n",
    "    \n",
    "    with similarity_stats_tab:\n",
    "        print('The Similarity Statistics will allow insights such as the number of records with high similarity, number of records with low similarity, histogram of records based on Similarity Scores as well as other insights.')\n",
    "        print('This could be used when performing a search get a similarity score per year (or Time such as Quarter). The document can be plotted using the categories. Where each is a potential precursor to the previous. For the similarity could let the user set a threshold (e.g., 0.1) to count the data. The threshold will filter by report score and then can use it to plot counts per year and for each category. Could plot the cummulative similarity across instead of counts. To validate could use the description of an accident and try to find similar records. Should see a large 1.0 in the accident and if there are many records related should also see the spike.')\n",
    "    with networkx_tab:\n",
    "        print('The Network Analysis will provide insights of how the records are connected and allow to filter based on a threshold.')\n",
    "\n",
    "    with summarization_tab:\n",
    "        print('The Summarization uses Convolutional Neural Network, DistilBART and Sentence Transformer libraries to develop AI based summaries of all the selected records, rows and/or reports.')\n",
    "        print('Reference: https://huggingface.co/philschmid/distilbart-cnn-12-6-samsum')\n",
    "        print('Example summarizationa at: movie_plots_summarization.ipynb')\n",
    "        print('Could be used to input a long movie plot description and make it shorter.')\n",
    "        \n",
    "    with filtered_reports_tab:        \n",
    "        display(HTML(f'<h4>Show Filtered Reports<h4>'))\n",
    "        print('Please select a data features to show and start and end rows to show records. \\nNotes:')\n",
    "        print('(1) Re-run the reports module to make sure all features appear as options. Some modules create features and columns (e.g., \"cluster_label\").')\n",
    "        print('(2) May be slow if a lot of records are selected.')\n",
    "        print('(3) Easier to read if less than 5 columns are selected.')\n",
    "              \n",
    "        filtered_reports_widget()\n",
    "        export_df_widget()\n",
    "        \n",
    "    with recluster_tab:       \n",
    "        display(HTML(f'<h3>Recluster Module<h3>'))\n",
    "        print('This widget allow to filter a specific cluster number and and use that filtered dataset with all the other analysis tools including clustering. This allows to perform infinite levels of clustering until running out of records.')\n",
    "        display(HTML(f'<h3>Recluster Process Steps<h3>'))\n",
    "        print('1. Run the \"Text Analysis and Modeling\" module with either Kmeans or DBSCAN and make note of which cluster you would like to \"recluster\".')\n",
    "        print('2. Select \"Run\" below.')\n",
    "        print('3. Code will ask which Cluster to select and select \"Run\" again.')\n",
    "        print('4. The data will be replaced with the cluster. To check go to reports and all the reporst should have the selected cluster number under the \"cluster_label\" feature (i.e., the last column).')\n",
    "        print('5. To reset run the main filter widget.')\n",
    "        recluster_widget()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered Dataframe: Unique Values Dictionary\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "The DF Unique Values Dictornary is used for stacking plots. After filtering the main dataframe, the data dictionary recalculates unique values and then uses those for the stacking. Stacking can be used with columns and features that have small number of unique values (e.g., categorical, one-hot-encoding, etc.). For continuous values there would be too many unique values and may need to convert to a bins in order to be able to use it in stacking plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_unique_values_dict():\n",
    "    global DATA_dict\n",
    "    \n",
    "    # Note could do a dictionary of 'feature' and 'list' by looping thru the data features. \n",
    "    # Could also find a mapping of description to features and map it to my data file features.\n",
    "    DATA_dict = {'description': ['None',\n",
    "                                 #'Genres',\n",
    "                                 'Original Language',\n",
    "                                 'Action',\n",
    "                                 'Drama',\n",
    "                                 'Release Calendar Year',],\n",
    "                'feature': ['None',\n",
    "                            #'genres',\n",
    "                            'original_language',\n",
    "                            'Action',\n",
    "                            'Drama',\n",
    "                            'release_cy',],\n",
    "                'list': [[], \n",
    "                         #sorted([x for x in df_data_filtered['genres'].unique()]), # Genres can't be used for \n",
    "                         sorted([x for x in df_data_filtered['original_language'].unique()]),\n",
    "                         sorted([x for x in df_data_filtered['Action'].unique()]),\n",
    "                         sorted([x for x in df_data_filtered['Drama'].unique()]),\n",
    "                         sorted([x for x in df_data_filtered['release_cy'].unique()]),\n",
    "                        ]\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics Widget Tab\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "References: <br>\n",
    "- Addition/Count of Non-Zeros: https://stackoverflow.com/questions/26053849/counting-non-zero-values-in-each-column-of-a-dataframe-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_title_and_records_widget():\n",
    "    interact_manual(stats_title_and_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_title_and_records():\n",
    "    # TITLE of tab and record numbers when NO reclustering applied. \n",
    "    if reclustering_counter == 0:\n",
    "        if (len(df_data) == len(df_data_filtered)):\n",
    "            display(HTML(f'<h2>Data Statistics and Plots for ALL Records<h2>'))\n",
    "            print(f'The source data has {len(df_data)} records and ALL {len(df_data_filtered)} records are selected.')\n",
    "        if (len(df_data) > len(df_data_filtered)):\n",
    "            display(HTML(f'<h2>Data Statistics and Plots for PARTIAL records (see selection filters)<h2>'))\n",
    "            print(f'The source data has {len(df_data)} records and ONLY {len(df_data_filtered)} records are selected.')\n",
    "            \n",
    "    # TITLES or tab and number of records when reclustering is applied\n",
    "    if reclustering_counter != 0:\n",
    "        display(HTML(f'<h2>Data Statistics for Recluster #{reclustering_counter}<h2>'))\n",
    "        print(f'Total number of records in the source data is {len(df_data)}.')\n",
    "        print(f'Total Number of records in the recluster data is {len(df_data_filtered)}.')\n",
    "        \n",
    "    # PLOTS\n",
    "    display(HTML(f'<h3>Records per Fiscal Year Start/End and Length<h3>'))\n",
    "    print(f'Note 1: Total number of selected records is {df_data_filtered.shape[0]}.\\n')\n",
    "    print(f'Note 2: When selecting a trendline the dashboard remove records that have unassigned time periods (e.g., CY, FY, etc.) if any.')\n",
    "    records_per_time_widget()\n",
    "\n",
    "    display(HTML(f'<h3>Record Statisticts Vertical Bar Plots<h3>'))\n",
    "    display(HTML(f'<h3>NOTE: THIS WOULD BE A GREAT EXAMPLE TO NORMALIZE.<h3>'))\n",
    "    print('Data for feature counts is sorted by descending order.')\n",
    "    feature_counts_widget()\n",
    "        \n",
    "    display(HTML(f'<h3>Record Statistics Horizontal Stacked Visualization<h3>'))\n",
    "    display(HTML(f'<h3>NOTE: NEED TO ALSO DO RELATIVE TO 100%.<h3>'))\n",
    "    barh_stats_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "Input Data Filtering and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_bow_rank_df(text, word_reduction_method, threshold): # Function only returns the top value.\n",
    "    global cosine_value, inputtext, df_bow, df_data_filtered # Making global value so that it can be called outside of the function.\n",
    "   \n",
    "    inputtext = str(text)\n",
    "    inputtext_normalized = text_normalization(inputtext, word_reduction_method) # Executing function to perform text normalization\n",
    "    bow = vectorizer.transform([inputtext_normalized]).toarray() # applying bow\n",
    "    # Calculating the cosine_value of the input_text against every Target Text Row (i.e., Question) in the datafram.\n",
    "    cosine_value = 1- pairwise_distances(df_bow, bow, metric = 'cosine')\n",
    "\n",
    "    # Defines and creates table for ranks\n",
    "    df_cosine = np.round(cosine_value, 4)\n",
    "    # Converting array to a pandas dataframe (table with index).\n",
    "    df_cosine_table = pd.DataFrame({'cosine_value': df_cosine[:, 0]})\n",
    "    # Concatenating the dataframes to show the results for Cosine Value (given inputtext), Original Data, norm Text and BoW\n",
    "    df_data_filtered = pd.concat([df_cosine_table, df_data_filtered], axis=1)\n",
    "    # Sorting values with highest Cosine Value on the Top and removing records below threshold\n",
    "    df_data_filtered = df_data_filtered.sort_values(by=['cosine_value'], ascending=False)\n",
    "    df_data_filtered = df_data_filtered[df_data_filtered['cosine_value'] > threshold].reset_index(drop = True)\n",
    "    df_data_filtered.insert(0, \"Input_Text\", inputtext, True)\n",
    "    \n",
    "    return (df_data_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def records_per_time_widget():\n",
    "    interact(records_per_time_data,\n",
    "             feature_to_plot = widgets.Dropdown(options = ['release_cy'],\n",
    "                                                 value = 'release_cy',   \n",
    "                                                 description = 'Records per',\n",
    "                                                 style={'description_width': 'initial'},\n",
    "                                                 disabled = False),\n",
    "             add_trend = widgets.Dropdown(options = ['None', 'Linear'],\n",
    "                                          value = 'None',   \n",
    "                                          description = 'Trendline (for unstacked plots)',\n",
    "                                          style={'description_width': 'initial'},\n",
    "                                          disabled = False),\n",
    "             add_equation = widgets.Checkbox(value = False,\n",
    "                                             description = 'Show Trendline Equation',\n",
    "                                             disabled = False,\n",
    "                                             Indent = False),\n",
    "             stack_by_desc = widgets.Dropdown(options = DATA_dict['description'],\n",
    "                                              value = 'None',   \n",
    "                                              description = 'Stack By',\n",
    "                                              style={'description_width': 'initial'},\n",
    "                                              disabled = False)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function creates stacked data plots.\n",
    "def records_per_time_data(feature_to_plot, add_trend, add_equation, stack_by_desc):\n",
    "    global df_time_record_counts\n",
    "    \n",
    "    stack_by_list = DATA_dict['list'][DATA_dict['description'].index(stack_by_desc)] \n",
    "    # Accessing the list from dictionary given the feature\n",
    "    stack_by = DATA_dict['feature'][DATA_dict['description'].index(stack_by_desc)]\n",
    "    \n",
    "    if (stack_by == 'None'):\n",
    "        df_time_record_counts = df_data_filtered[feature_to_plot].value_counts().sort_index(ascending = True,\n",
    "                                                                                           ).rename_axis(feature_to_plot,\n",
    "                                                                                                        ).reset_index(name='counts')\n",
    "        if (add_trend == 'Linear') & ((feature_to_plot == 'release_cy')): \n",
    "            # For adding a trendline need to remove the null values and change the data type to float.\n",
    "            df_time_record_counts = df_time_record_counts.loc[(df_time_record_counts[feature_to_plot] != 'NaT') &\n",
    "                                                              (df_time_record_counts[feature_to_plot] != np.nan)]\n",
    "            df_time_record_counts[feature_to_plot] = df_time_record_counts[feature_to_plot].astype(float)\n",
    "        \n",
    "            bar_chart_wtrend(input_data = df_time_record_counts, x_feature = feature_to_plot, y_feature = 'counts', \n",
    "                             add_trend = add_trend, add_equation = add_equation)\n",
    "        \n",
    "        else: #if (add_trend == 'Linear') & ((feature_to_plot != 'release_cy') | (feature_to_plot != 'DATE_FY')):\n",
    "            print(f'NOTE: Linear Trend is not setup to calculate trendline for the Quarters or Months yet.')\n",
    "            bar_chart_wtrend(input_data = df_time_record_counts, x_feature = feature_to_plot, y_feature = 'counts', \n",
    "                             add_trend = 'None', add_equation = add_equation)\n",
    "                                                        \n",
    "    else:\n",
    "        stacked_bar_plot_data(input_data = df_data_filtered, feature_to_plot = feature_to_plot, \n",
    "                              stack_by = stack_by, stack_by_list = stack_by_list)\n",
    "        \n",
    "    # NOTE THAT THE EXCHANGE DASHBOARDS HAVE OTHER FEATURE PLOTS HERE SUCH AS ACTIVE PROJECTS AND PROJECT LENGHTS IF NEEDED.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Plotting Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_bar_plot_data(input_data, feature_to_plot, stack_by, stack_by_list):\n",
    "    df_data_stacked = input_data[[feature_to_plot, \n",
    "                                  stack_by]].pivot_table(index=[feature_to_plot], \n",
    "                                                         columns=[stack_by], \n",
    "                                                         aggfunc=len,\n",
    "                                                         dropna=False,\n",
    "                                                         fill_value=0)\n",
    "    df_data_stacked.reset_index(level=0, inplace=True)\n",
    "    df_data_stacked[\"Row_Total\"] = df_data_stacked[list(stack_by_list)].sum(axis=1)\n",
    "    stacked_bar_plot(input_data = df_data_stacked, feature_to_plot = feature_to_plot, \n",
    "                     stack_by = stack_by, stack_by_list = stack_by_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_bar_plot(input_data, feature_to_plot, stack_by, stack_by_list):\n",
    "    fig_width = max(14, (int(len(input_data[feature_to_plot].unique())/4))) \n",
    "    # Adjust the width of the figure dynamically depending on number of unique values in the X axis.\n",
    "    ax = input_data.plot.bar(x = feature_to_plot, \n",
    "                        y = stack_by_list, \n",
    "                        stacked=True, \n",
    "                        figsize=(fig_width, 4))\n",
    "    plt.legend(reversed(plt.legend().legendHandles), reversed(stack_by_list),\n",
    "               loc=\"upper left\", bbox_to_anchor=(1, 1), ncol=1)\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.ylabel('No. of Records', size=16)\n",
    "    plt.grid(axis = 'y')\n",
    "    \n",
    "    # Write values inside stacked bars\n",
    "    for rect in ax.patches: # .patches is everything inside of the chart\n",
    "        # Find where everything is located\n",
    "        height = int(rect.get_height())\n",
    "        width = rect.get_width()\n",
    "        x = rect.get_x()\n",
    "        y = rect.get_y()\n",
    "        # The height of the bar is the data value and can be used as the label\n",
    "        label_text = f'{height}'  # f'{height:.2f}' to format decimal values\n",
    "        # ax.text(x, y, text)\n",
    "        label_x = x + width / 2\n",
    "        label_y = y + height / 2\n",
    "        if height > 15: # Write value on plot only when height is greater than specified value\n",
    "            ax.text(label_x, label_y, label_text, ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_chart_wtrend(input_data, x_feature, y_feature, add_trend, add_equation):\n",
    "    fig_width = max(14, (int(len(input_data[x_feature].unique())/4))) \n",
    "    # Adjust the width of the figure dynamically depending on number of unique values in the X axis.\n",
    "    plt.figure(figsize=(fig_width, 4))\n",
    "    plt.bar(x = input_data[x_feature], height = input_data[y_feature],) # Bars variable to access bar attributes\n",
    "    ax = plt.gca()    \n",
    "    plt.xlabel(x_feature, fontsize=12)\n",
    "    if len(input_data) > 10:\n",
    "        plt.xticks(ticks = input_data[x_feature].unique(), rotation = 90, size = 14)\n",
    "    else:\n",
    "        plt.xticks(ticks = input_data[x_feature].unique(), rotation = 0, size = 14)  \n",
    "    plt.ylabel(\"No. of Records\", fontsize=16)\n",
    "    if (add_trend != 'None'):\n",
    "        ax.ticklabel_format(useOffset=False) \n",
    "        # Fixes an issue with the x-axis showing as integers and +202X in the clustering trends. \n",
    "        # Using the if allows this function to be used with the 'NaT' \n",
    "    plt.yticks(size = 14)\n",
    "    plt.ylim(bottom = 0)\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.grid(axis = 'y')   \n",
    "    \n",
    "    for i in range(len(input_data)):\n",
    "        # Prints values on plot\n",
    "        if len(input_data) > 10:\n",
    "            plt.text(input_data[x_feature][i], \n",
    "                     input_data[y_feature][i]+input_data[y_feature].max()*.01, \n",
    "                     input_data[y_feature][i], \n",
    "                     size = 14,\n",
    "                     ha='center',\n",
    "                     rotation = 90)\n",
    "        else:\n",
    "            plt.text(input_data[x_feature][i], \n",
    "                     input_data[y_feature][i]+input_data[y_feature].max()*.01, \n",
    "                     input_data[y_feature][i], \n",
    "                     size = 14,\n",
    "                     ha='center',\n",
    "                     rotation = 0)\n",
    "            \n",
    "    # Linear Trendline\n",
    "    if (add_trend == 'Linear'): \n",
    "        add_LR_trend(input_data, x_feature, y_feature, add_trend, add_equation)\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_LR_trend(input_data, x_feature, y_feature, add_trend, add_equation):\n",
    "    # Linear Trendline equation\n",
    "    df_freq_fit = np.polynomial.polynomial.polyfit(input_data[x_feature], input_data[y_feature], 1)\n",
    "    y_intercept_min_year = df_freq_fit[1]*input_data[x_feature].min()+df_freq_fit[0]\n",
    "    \n",
    "    # Drawing the linear trendline\n",
    "    plt.plot(input_data[x_feature], df_freq_fit[1] * input_data[x_feature] + df_freq_fit[0], color='red', linewidth=2)\n",
    "    if add_equation == True:\n",
    "        plt.text((input_data[x_feature].max()+1.5), \n",
    "                 (input_data[y_feature].min()+input_data[y_feature].max()*0.1), \n",
    "                 'Trendline Equation: y={:.2f}*x+{:.2f}'.format(df_freq_fit[1], y_intercept_min_year), \n",
    "                 color='darkblue', \n",
    "                 size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_counts_widget():\n",
    "    interact(feature_counts_data,\n",
    "             x_feature = widgets.Dropdown(options = DATA_dict['feature'][1:],\n",
    "                                          value = DATA_dict['feature'][1],\n",
    "                                          description = 'Feature to Plot',\n",
    "                                          style={'description_width': 'initial'},\n",
    "                                          disabled = False),\n",
    "             Pareto_chkbox = widgets.Checkbox(value = False,\n",
    "                                              description = 'Pareto Chart',\n",
    "                                              disabled = False,\n",
    "                                              Indent = False),\n",
    "             Pareto_Axs_chkbox = widgets.Checkbox(value = True,\n",
    "                                                  description = 'Pareto Chart Y-axis',\n",
    "                                                  disabled = False,\n",
    "                                                  Indent = False),\n",
    "             Sort_by_feature_name_chkbox = widgets.Checkbox(value = False,\n",
    "                                                     description = 'Sort by Feature Name',\n",
    "                                                     disabled = False,\n",
    "                                                     Indent = False)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_counts_data(x_feature, Pareto_chkbox, Pareto_Axs_chkbox, Sort_by_feature_name_chkbox):\n",
    "    global df_feature_counts\n",
    "    \n",
    "    if Sort_by_feature_name_chkbox == False:\n",
    "        df_feature_counts = df_data_filtered[x_feature].value_counts().rename_axis(x_feature).reset_index(name='counts')\n",
    "    if Sort_by_feature_name_chkbox == True:\n",
    "        df_feature_counts = df_data_filtered[x_feature].value_counts().rename_axis(x_feature).reset_index(name='counts').sort_values(x_feature)\n",
    "\n",
    "    bar_chart_wpareto(input_data = df_feature_counts, x_feature = x_feature, \n",
    "                      Pareto_chkbox = Pareto_chkbox, Pareto_Axs_chkbox = Pareto_Axs_chkbox)\n",
    "    \n",
    "    #horizontal_bar_chart(input_data = df_location_counts.sort_values(by = 'counts'), y_feature = 'counts', x_feature = x_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location based count bar plot with Pareto\n",
    "def bar_chart_wpareto(input_data, x_feature, Pareto_chkbox, Pareto_Axs_chkbox):\n",
    "    global ax1    \n",
    "    fig_width = max(14, (int(len(input_data[x_feature].unique())/3)))\n",
    "    fig, ax1 = plt.subplots(figsize=(fig_width, 4))\n",
    "    plt.bar(x = input_data[x_feature], height = input_data['counts'])\n",
    "    plt.xlabel(f'{x_feature}', fontsize=12)\n",
    "    plt.xticks(ticks = input_data[x_feature].unique(), rotation = 90, size = 14)\n",
    "    plt.ylabel(\"No. of Records\", fontsize=16)\n",
    "    plt.yticks(size = 14)\n",
    "    plt.ylim(bottom = 0)\n",
    "    #plt.gca().yaxis.set_major_formatter(int_number_format) # Calls the function and formats the y-axis.\n",
    "    ax1 = plt.gca()\n",
    "    ax1.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax1.grid(axis = 'y')\n",
    "    \n",
    "    # Show the values of each bar\n",
    "    #if len(input_data[x_feature].unique()) <= 15:\n",
    "    for i in range(len(input_data)):\n",
    "        # Prints values on plot\n",
    "        plt.text(input_data[x_feature][i], \n",
    "                 input_data['counts'][i]+input_data['counts'].max()*0.01, \n",
    "                 int_number_format(input_data['counts'][i], 0), \n",
    "                 size = 14)\n",
    "        \n",
    "    # PARETO CHART\n",
    "    if Pareto_chkbox == True:\n",
    "        # PARETO CHART Data for Plot \n",
    "        x = input_data[x_feature].values\n",
    "        y = input_data['counts'].values\n",
    "        pareto_chart(x, y, ax1, Pareto_Axs_chkbox)\n",
    "    \n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_chart(x, y, ax, Pareto_Axs_chkbox):\n",
    "    global ax2\n",
    "    weights = y / y.sum()\n",
    "    cumsum = weights.cumsum()\n",
    "\n",
    "    # Drwaing Pareto Chart on Secondary Y-Axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, cumsum, color = 'green', marker = 'o', linestyle = 'dashed')\n",
    "    ax2.set_ylabel('', color='g')\n",
    "    ax2.tick_params('y', colors='g')\n",
    "    ax2.yaxis.set_ticks(np.arange(0, 1.1, 0.20))\n",
    "    ax2.set_yticklabels(['{:,.0%}'.format(x) for x in ax2.get_yticks().tolist()])\n",
    "    ax2.axhline(0.80, color=\"orange\", linestyle=\"dashed\") # Horizontal Line at 80%.\n",
    "        \n",
    "    # Y-labels on Secondary Y-Axis (i.e., right side)        \n",
    "    if Pareto_Axs_chkbox == False:\n",
    "        ax2.set_yticks([])\n",
    "        formatted_weights = ['{0:.0%}'.format(x) for x in cumsum]\n",
    "        for i, txt in enumerate(formatted_weights):\n",
    "            ax2.annotate(txt, (x[i], cumsum[i]), color = 'black', fontweight='heavy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_bar_chart(input_data, y_feature, x_feature):\n",
    "    fig_height = max(10, (int(len(input_data[x_feature].unique())/4)))\n",
    "    plt.figure(figsize=(4, fig_height))\n",
    "    plt.barh(y = input_data[x_feature], width = input_data[y_feature])\n",
    "    for index, value in enumerate(input_data[y_feature]):\n",
    "        plt.text(value, index, str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barh_stats_widget():\n",
    "    interact_manual(barh_stats_stack_widget,\n",
    "                    x_feature_W = widgets.Dropdown(options = DATA_dict['feature'][1:],\n",
    "                                          value = DATA_dict['feature'][1],   \n",
    "                                          description = 'Feature to Plot',\n",
    "                                          style={'description_width': 'initial'},\n",
    "                                          disabled = False)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barh_stats_stack_widget(x_feature_W):\n",
    "    global x_feature\n",
    "    x_feature = x_feature_W\n",
    "    \n",
    "    # Formula removes the x_feature from list dynamically.\n",
    "    stack_by_list = DATA_dict['feature'][1:][:DATA_dict['feature'][1:].index(x_feature)] + DATA_dict['feature'][1:][DATA_dict['feature'][1:].index(x_feature)+1:]\n",
    "    \n",
    "    interact(barh_stats_sort_widget,\n",
    "             stack_by_W = widgets.Dropdown(options = stack_by_list,\n",
    "                                           value = stack_by_list[0],   \n",
    "                                           description = 'Stack By',\n",
    "                                           style={'description_width': 'initial'},\n",
    "                                           disabled = False)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barh_stats_sort_widget(stack_by_W):\n",
    "    global stack_by\n",
    "    stack_by = stack_by_W\n",
    "    \n",
    "    interact(barh_stats_data,           \n",
    "             sort_first = widgets.Dropdown(options = ['None']+DATA_dict['list'][DATA_dict['feature'].index(stack_by)],\n",
    "                                           value = 'None',  \n",
    "                                           description = 'First Level Sort',\n",
    "                                           style={'description_width': 'initial'},\n",
    "                                           disabled = False),\n",
    "             sort_second = widgets.Dropdown(options = ['None']+DATA_dict['list'][DATA_dict['feature'].index(stack_by)],\n",
    "                                            value = 'None',   \n",
    "                                            description = 'Second Level Sort',\n",
    "                                            style={'description_width': 'initial'},\n",
    "                                            disabled = False)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barh_stats_data(sort_first, sort_second):\n",
    "    global labels_list, df\n",
    "    # DATA TRANSFORMATION FOR BAR PLOT WITH PRINTED VALUES\n",
    "    df_pivot = df_data_filtered[[x_feature]+[stack_by]].pivot_table(index=x_feature,\n",
    "                                                                    columns = stack_by, \n",
    "                                                                    aggfunc=len,\n",
    "                                                                    dropna=False,\n",
    "                                                                    fill_value=0,\n",
    "                                                                    margins=True).reset_index()\n",
    "    # SORTS THE FEATURES AND COLUMNS TO BE USED IN THE PLOTS \n",
    "    sort_list = ['None']+DATA_dict['list'][DATA_dict['feature'].index(stack_by)]\n",
    "    sorted_columns_list = [sort_first, sort_second]+sort_list+['All']\n",
    "    sorted_columns_list = list(dict.fromkeys([element for element in sorted_columns_list if element != 'None'])) # Removes 'None' and Duplicates\n",
    "    sorted_columns_list.insert(0, x_feature)\n",
    "\n",
    "    df_pivot = df_pivot[sorted_columns_list] # Sorts all dataframe features in the right order.\n",
    "    df_pivot = df_pivot.drop([len(df_pivot)-1]) # Drops last \"All\" row\n",
    "\n",
    "    # SORTING THE DATA FOR THE PLOTS\n",
    "    if (sort_first == 'None') & (sort_second == 'None'): # Sorts in this order by Totals if none order specified.\n",
    "        df_pivot = df_pivot.sort_values(['All']) \n",
    "    if (sort_first != 'None') & (sort_second == 'None'): # Sorts by specified one level\n",
    "        df_pivot = df_pivot.sort_values([sort_first])   \n",
    "    if (sort_first != 'None') & (sort_second != 'None'): # Sorts by the specieifed two levels\n",
    "        df_pivot = df_pivot.sort_values([sort_first]+[sort_second]) \n",
    "  \n",
    "    df = df_pivot.drop(columns = 'All', axis = 0) # Drops last \"All\" column\n",
    "    df_total = df_pivot['All']\n",
    "    df_percent = df[df.columns[1:]].div(df_total, 0)*100\n",
    "    \n",
    "    # DEVELOPING THE LABELS LIST FROM THE SORTED COLUMNS AND REMOVING THE X_FEATURE AND ALL COLUMNS  \n",
    "    labels_list = sorted_columns_list\n",
    "    labels_list.remove(x_feature)\n",
    "    labels_list.remove('All')\n",
    "\n",
    "    barh_stats_chart(input_data = df, input_data_pivot = df_pivot, input_data_total = df_total,\n",
    "                     input_data_percent = df_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barh_stats_chart(input_data, input_data_pivot, input_data_total, input_data_percent):\n",
    "    # HORIZONTAL BAR PLOT WITH LABELS\n",
    "    print('Note 1: When stacking, percentage is shown for stack bars with 10 or more records and less than 100%.')\n",
    "    print('Note 2: Plot with undefined sorting level is performed on the total number of records.')\n",
    "\n",
    "    # figure and axis\n",
    "    fig_height = max(2, min(30, (int(len(input_data_pivot)*0.8))))\n",
    "    fig, ax = plt.subplots(1, figsize=(12, fig_height))\n",
    "    # plot bars\n",
    "    left = len(input_data_pivot) * [0]\n",
    "    for idx, name in enumerate(labels_list):\n",
    "        plt.barh(y = input_data_pivot[x_feature], width = input_data_pivot[name], height = 0.9, left = left)\n",
    "        left = left + input_data_pivot[name]\n",
    "    # title, legend, labels\n",
    "    plt.title('', loc='left')\n",
    "    plt.legend(labels_list, bbox_to_anchor=([1, 1]), frameon=False)\n",
    "    plt.xlabel('Number of Records')\n",
    "    # remove spines\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    # adjust limits and draw grid lines\n",
    "    plt.ylim(-0.5, ax.get_yticks()[-1] + 0.5)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.xaxis.grid(color='gray', linestyle='dashed')\n",
    "\n",
    "    va = ['top', 'bottom']\n",
    "    va_idx = 0\n",
    "    \n",
    "    for n in input_data_percent:\n",
    "        va_idx = 1 - va_idx\n",
    "        # cs = cumulative sum, ab = absolute number of records, pc = percent of total records, tot = total.\n",
    "        for i, (cs, ab, pc, tot) in enumerate(zip(input_data.iloc[:, 1:].cumsum(1)[n], \n",
    "                                                  input_data[n], \n",
    "                                                  input_data_percent[n], \n",
    "                                                  input_data_total)):\n",
    "            # TOTAL FOR EACH BAR.\n",
    "            plt.text(tot+1, i, str(tot), va='center') # Shows the Total of the Bar.\n",
    "            \n",
    "            # PERCENTAGE OF STACKED BARS GIVEN THE CONDITIONS.\n",
    "            if (ab >= 30) & (pc < 100): # Shows value horizontally if the absolute value is >=30 or percentage is < 100.\n",
    "                 plt.text(cs - ab/2, i, str(int(pc)) + '%', va='center', ha='center')\n",
    "            if (30 > ab) & (ab >= 10) & (pc < 100): # Shows value rotated if the absolute value is <30 and >10 and percentage is <100.\n",
    "                plt.text(cs - ab/2, i, str(int(pc)) + '%', va='center', ha='center', rotation=90)\n",
    "            # If total is 100% it does not show the value.\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis and Text Modeling Widget\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "The text analysis and modeling algorithms perform several types of analyses. This include Latent Derechlit Allocation (LDA) which is a type of topic model as well as text clustering using Kmeans and DBSCAN algorithms. These algorithms provide insights by clustering reports into similar groups of reports and givin statistics on the top terms.\n",
    "\n",
    "LDA: \n",
    "\n",
    "KMEANS: In the Kmeans algorithm the user specifies number of clusters. With this parameter the algorithm separates the records in the specified number of clusters. The number of clusters can also include an optimal number of clusters where the distance between two datapoints is optimal for the number of clusters.\n",
    "\n",
    "DBSCAN: The DBSCAN algorithm uses to main parameters EPS and minimum samples in a cluster. The EPS value is the required distance between two datapoints to be considered a cluster. The minimum samples in a cluster is the required number of datapoints to be considered a cluster. Note that low minimum sample in clusters (e.g., 2) can result in a large number of clusters.\n",
    "\n",
    "Dimensionality Reduction: Clustering algorithms (non-LDA algorithms) can breakdown with datasets with high dimensionality and may not provide meaningful clusters. Dimensionality reduction techniques (e.g., PCA and TSNE) are used in combination with the clustering algorithms (i.e., Kmeans and DBSCAN) to project high dimensions to lower dimensions (e.g., 2D, 3D, etc.) to identify clusters. The scatter plots here are a 2D projections based on TSNE and PCA mathematical techniques. In the case of TSNE, it naturally expands dense clusters, and contracts sparse ones, evening out cluster sizes given the Perplexity parameter.\n",
    "\n",
    "Recommendation: Based on experience, to identify meaningful cluster and high relationship to the terms it is better to use term score. For example, TFIDF scores above 0.10 are considered to have good relationship to the cluster. Lower values can also represent good relationship as long as several terms are related. To explore terms and topics it is recommended to run Kmeans with no Dimensionality Reduction and DBSCAN with both PCA and TSNE.\n",
    "\n",
    "References: <br>\n",
    "- Random States: https://towardsdatascience.com/manipulating-machine-learning-results-with-random-state-2a6f49b31081\n",
    "- Random States: https://scikit-learn.org/stable/faq.html#how-do-i-set-a-random-state-for-an-entire-execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_modelling_widget():\n",
    "    interact_manual(text_modelling, \n",
    "                    text_model_W = widgets.Dropdown(options=TEXT_MODEL_list,\n",
    "                                                            value = 'None',\n",
    "                                                            description = 'Text Analysis Method', \n",
    "                                                            disabled=False, style={'description_width': 'initial'},\n",
    "                                                            layout=widgets.Layout(width='40%')),\n",
    "                    vect_W = widgets.Dropdown(options= ['TFIDF', 'Count'], \n",
    "                                              description = 'Word Vectorizer',\n",
    "                                              value = 'TFIDF',\n",
    "                                              disabled=False, style={'description_width': 'initial'},\n",
    "                                              layout=widgets.Layout(width='40%')),\n",
    "                    word_preprocessing_W = widgets.Dropdown(options= ['Lemmatization', 'Stemming'], \n",
    "                                                                       description = 'Word Preprocessing Method',\n",
    "                                                                       value = 'Lemmatization',\n",
    "                                                                       disabled=False, style={'description_width': 'initial'},\n",
    "                                                                       layout=widgets.Layout(width='40%')),\n",
    "                    dimensionality_reduction_W = widgets.Dropdown(options= ['None', 'PCA', 'TSNE'], \n",
    "                                                                       description = 'Dimensionality Reduction Method',\n",
    "                                                                       value = 'PCA',\n",
    "                                                                       disabled=False, style={'description_width': 'initial'},\n",
    "                                                                       layout=widgets.Layout(width='40%')),\n",
    "                    max_df_W = widgets.FloatSlider(min=0.05, \n",
    "                                                   max=1.0, \n",
    "                                                   value=0.95, \n",
    "                                                   step=0.05,\n",
    "                                                   description=\"Term Maximum Document Frequency\",\n",
    "                                                   disables=False, style={'description_width': 'initial'},\n",
    "                                                   layout=widgets.Layout(width='50%'),\n",
    "                                                   readout_format='.2f',),\n",
    "                    min_df_W = widgets.FloatSlider(min=0.0001, \n",
    "                                                   max=0.1, \n",
    "                                                   value=0.01, \n",
    "                                                   step=0.0005,\n",
    "                                                   description=\"Term Minimum Document Frequency\",\n",
    "                                                   disables=False, style={'description_width': 'initial'},\n",
    "                                                   layout=widgets.Layout(width='50%'),\n",
    "                                                   readout_format='.4f',),\n",
    "                    cluster_two_D_plot_W = widgets.Dropdown(options= ['None', 'PCA', 'TSNE'], \n",
    "                                                            description = 'Comparison 2D Plot',\n",
    "                                                            value = 'None',\n",
    "                                                            disabled=False, style={'description_width': 'initial'},\n",
    "                                                            layout=widgets.Layout(width='40%')),\n",
    "                    trend_timeperiod_W = widgets.Dropdown(options= ['None', 'release_cy'],\n",
    "                                                                  description = 'Cluster Trend by',\n",
    "                                                                  value = 'release_cy',\n",
    "                                                                  disabled=False, style={'description_width': 'initial'},\n",
    "                                                                  layout=widgets.Layout(width='40%')),\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis and Text Modeling Widget Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_modelling(text_model_W, vect_W, word_preprocessing_W, dimensionality_reduction_W, max_df_W, min_df_W, \n",
    "                   cluster_two_D_plot_W, trend_timeperiod_W):\n",
    "    global text_model, word_preprocessing, dimensionality_reduction, max_df, min_df, cluster_two_D_plot, bow_calc_array_scaled, recommended_min_samples_inacluster, trend_timeperiod\n",
    "    text_model = text_model_W\n",
    "    vect = vect_W\n",
    "    word_preprocessing = word_preprocessing_W\n",
    "    dimensionality_reduction = dimensionality_reduction_W\n",
    "    max_df = max_df_W\n",
    "    min_df = min_df_W\n",
    "    cluster_two_D_plot = cluster_two_D_plot_W\n",
    "    trend_timeperiod = trend_timeperiod_W\n",
    "    \n",
    "\n",
    "    bow_vectorizer_data(input_data = df_data_filtered, vect = vect_W, word_preprocessing = word_preprocessing,\n",
    "                        max_df = max_df, min_df = min_df)\n",
    "    \n",
    "    if text_model_W == 'None':\n",
    "        display(HTML(f'<h2>Please Select a Text Analysis Method to obtain results.<h2>'))\n",
    "    \n",
    "    # Dimensionality Reduction Functions \n",
    "    # Needs to be called before the clustering functions as these functions create the bow_calc_array_scaled\n",
    "    if dimensionality_reduction_W == 'None':\n",
    "        bow_calc_array_scaled = bow_array\n",
    "        \n",
    "    if dimensionality_reduction_W == 'PCA':\n",
    "        pca_variance_plot()\n",
    "        PCA_func()\n",
    "        \n",
    "    if dimensionality_reduction_W == 'TSNE':\n",
    "        TSNE_func()\n",
    "    \n",
    "    # Kmeans Clustering\n",
    "    if text_model_W == 'Clustering: Kmeans (user defined clusters)':\n",
    "        kmeans_k_clusters_widget()\n",
    "        \n",
    "    if text_model_W == 'Clustering: Kmeans w/Optimal K':\n",
    "        kmeans_optimal_k_widget()\n",
    "        kmeans_k_clusters_widget()\n",
    "    \n",
    "    # DBSCAN Clustering\n",
    "    if text_model_W == 'Clustering: DBSCAN (Max Clusters w/Optimal EPS)':\n",
    "        distances_func()\n",
    "        dbscan_max_clusters_opt_eps_widget()\n",
    "    \n",
    "    if text_model_W == 'Clustering: DBSCAN (Densest Clusters w/Optimal EPS)':\n",
    "        distances_func()\n",
    "        dbscan_dense_clusters_opt_eps_widget()\n",
    "    \n",
    "    if text_model_W == 'Clustering: DBSCAN (% Records to Cluster)':\n",
    "        recommended_min_samples_inacluster = min(max(np.ceil(df_data_filtered.shape[0]*0.005), 2), 500) \n",
    "        # Used as Widget Input for the recommended minimum number of reports in cluster \n",
    "        # (i.e., the minimum of either 500 or the largest of 0.5% of total rows and 2).\n",
    "        distances_func()\n",
    "        dbscan_cluster_by_percent_widget()\n",
    "        \n",
    "    if text_model_W == 'Clustering: DBSCAN (Custom EPS and Min Samples)':\n",
    "        recommended_min_samples_inacluster = min(max(np.ceil(df_data_filtered.shape[0]*0.005), 2), 500) \n",
    "        # Used as Widget Input for the recommended minimum number of reports in cluster \n",
    "        # (i.e., the minimum of either 500 or the largest of 0.5% of total rows and 2).\n",
    "        distances_func()\n",
    "        dbscan_cluster_custom_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Text Model Function \n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "References: <br>\n",
    "- Explanation of min_df and max_df in scikit vectorizers: https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_vectorizer_data(input_data, vect, word_preprocessing, max_df, min_df):\n",
    "    global bow_array, features, df_bow, vectorizer, stopwords_DATA\n",
    "    # DATA Specific Stopwords\n",
    "    stopwords_DATA = text_normalization(text = stopwords_to_add_L2+STOPWORDS_winput, \n",
    "                                        word_reduction_method = word_preprocessing).split()\n",
    "\n",
    "    # Creating BOW for the Target DataFrame with selected Vectorizer TFIDF\n",
    "    if vect == 'TFIDF':\n",
    "        vectorizer = TfidfVectorizer(lowercase=True, analyzer='word', stop_words=stopwords_DATA, \n",
    "                                     ngram_range=(1, 2), max_df=max_df, min_df=min_df) \n",
    "    if vect == 'Count':\n",
    "        vectorizer = CountVectorizer(lowercase=True, analyzer='word', stop_words=stopwords_DATA, \n",
    "                                     ngram_range=(1, 2), max_df=max_df, min_df=min_df) \n",
    "    # Text Preprocessing\n",
    "    if word_preprocessing == 'Lemmatization':\n",
    "        bow_array = vectorizer.fit_transform(input_data['norm_text_lemma']).toarray()\n",
    "    if word_preprocessing == 'Stemming':\n",
    "        bow_array = vectorizer.fit_transform(input_data['norm_text_stem']).toarray()\n",
    "\n",
    "    # Returns word vectors.\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    df_bow = pd.DataFrame(bow_array, columns = features)\n",
    "        \n",
    "    input_data_stats_desc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data_stats_desc():\n",
    "    print(f'The input data has {df_data_filtered.shape[0]} records.')\n",
    "    print(f'The {df_bow.shape[0]} records have {df_bow.shape[1]} words in the Bag of Words model.') \n",
    "    print(f'Data Specific and filter added stopwords: {stopwords_DATA}.')\n",
    "    print('NLTK stopwords are also used during cleaning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "There are two main methods for reducing the dimensionality of high dimensional data. These two are PCA and TSNE. When a dataset is high number of dimension and clustering will be performed it is typically recommended to use some form of dimensionality reduction. These methods find a mathematical representatation of multi-dimensionsional data (i.e., >3D) in three or two dimensions. In this case everyword within each record (e.g., movie title and overview) is a dimension.\n",
    "\n",
    "For both methods (i.e., PCA and TSNE) an array of the reduced dimensions for performing the clustering calculation is created (bow_calc_array). This reduces the dimensionality to 2 dimensions and is a projection from the full bag of words array (bow_array). The bow_calc_array is then scaled as bow_calc_array_scaled and used in the clustering algorithm. The bow_array include all terms and words (i.e., features). In this functions, the bow_calc_array is always used for the calculation of the clusters while the bow_array is always used for the determination of top words within a cluster. \n",
    "\n",
    "When no Dimensionality Reduction is used bow_calc_array = bow_array and allows to see the assignment of clusters within the PCA or TSNE projection. This allows for verification of correct behavior of the algorith. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Dimensionality Reduction Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "References: <br>\n",
    "- https://github.com/DhruvilKarani/PCA-blog-notebook/blob/master/PCA.ipynb\n",
    "- https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html\n",
    "- https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-7-unsupervised-learning-pca-and-clustering-db7879568417\n",
    "- https://towardsdatascience.com/explaining-k-means-clustering-5298dc47bad6\n",
    "- https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-kmeans-clustering-pca-b7ba6fafa74\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_variance_plot():\n",
    "    NUM_COMPONENTS = min(df_bow.shape[0], bow_array.shape[1]) # Number of components. Cannot be larger than the number of n_samples or features. df_bow.shape[0] is the number of samples (e.g., records) in the sample. The bow_array.shape[1] is the number of features (terms or words) in the dataframe. \n",
    "    pca = PCA(n_components = NUM_COMPONENTS) # Defining the PCA model variable\n",
    "    bow_calc_array = pca.fit_transform(bow_array) # The bow_calc_array is the reduced bow array to be used for the calculation of clusters.\n",
    "    variance_explained = np.cumsum(pca.explained_variance_)\n",
    "    \n",
    "    print('The y-axis is the fraction of cummulative explained variance given the number of components in the x-axis. Ideally we would select the number of components such that we can explain a high number of the variability. For example, if we wanted to explain 80% of the variability we would need number of components that match to 0.8. However, if the result is high it would probably not be feasible. For simplicity the PCA Dimensionality Reduction here reduces the dimensions to 2 components and can be visualized in a 2D plot.')\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    plt.plot(range(NUM_COMPONENTS),variance_explained, color='r')\n",
    "    ax.grid(True)\n",
    "    plt.title(label = f\"Cummulative Explained Variance vs. {NUM_COMPONENTS} Components\")\n",
    "    plt.xlabel(\"Number of components\")\n",
    "    plt.ylabel(\"Cumulative explained variance\")\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_func():\n",
    "    global bow_calc_array, bow_calc_array_scaled\n",
    "    pca = PCA(n_components = 2) # PCA Model with 2 Components\n",
    "    bow_calc_array = pca.fit_transform(bow_array) # The bow_calc_array is the reduced bow_array to be used for the calculation of clusters.\n",
    "\n",
    "    # Projection Plot (scaled) of records in data.\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(bow_calc_array)\n",
    "    bow_calc_array_scaled = scaler.transform(bow_calc_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE Dimensionality Reduction Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "References: <br>\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "- https://towardsdatascience.com/t-sne-clearly-explained-d84c537f53a\n",
    "- https://distill.pub/2016/misread-tsne/\n",
    "- https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1\n",
    "- https://towardsdatascience.com/high-dimension-clustering-w-t-sne-dbscan-dcec77e6a39b\n",
    "- https://link.medium.com/zNc9QSQj0cb\n",
    "- https://github.com/dougfoo/machineLearning/blob/master/covid/COVID-global-clustering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TSNE_func():\n",
    "    global perplexity, bow_calc_array, bow_calc_array_scaled\n",
    "    # TSNE with defined perplexity.  The perplexity defines how the TSNE Plot looks and its two main dimensions.  Ideally the user wants separable clusters and would iterate thru different perplexities and select the one with the most defined clusters.  In case of high variable data (e.g., different filters that could be applied), reduce dimensionality and users being SME's in other areas, it would be tough to evaluate and select a perplexity for each unique case.\n",
    "    perplexity = 30\n",
    "    bow_calc_array = TSNE(n_components=2, perplexity = perplexity, random_state=42, n_iter=5000).fit_transform(bow_array) # TSNE Model with 2 Componens\n",
    "    \n",
    "    # Projection Plot (scaled) of records in data.\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(bow_calc_array)\n",
    "    bow_calc_array_scaled = scaler.transform(bow_calc_array)\n",
    "    \n",
    "    print(f'PLEASE WAIT: Running TSNE function is slow.  TSNE Perplexity parameter is set at {perplexity}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Plots and Plot Projection Function\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "These functions are used to create the plots in the cluster including the projections of the data in 2-Dimensions. The projections can be used to verify that the clustering algorithm is behaving correctly. For example, when Dimensionality Reduction is used the projection plot should have each data point near them with the same color of cluster unless it is in the edge of the cluster. When no dimensionality reduction is used the expected behavior is that the data points for each cluster be overimposed within one another. That is because the clustering algorithm did not used the reduced dimensionality data but rather all the dimensions and is an expected behavior.\n",
    "\n",
    "This function and its corresponding PCA_func and TSNE_func are called at the end of the clustering algorithm in order to avoid affecting the bow_calc_array_scaled. In this case teh bow_calc_array_scaled is used for the projection but does not affect the calculation of clusters.\n",
    "\n",
    "References: <br>\n",
    "- Euclidean Distance: https://www.kite.com/python/answers/how-to-find-euclidean-distance-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_plots():\n",
    "    global cluster_pred\n",
    "    \n",
    "    cluster_pred = df_data_filtered['cluster_label_by_function'].to_numpy() # Converting cluster_label assigments to an array.\n",
    "    df_best_features = get_top_features_cluster(bow_array = bow_array, prediction = cluster_pred, n_feats = 20) # Gets top n_feats (e.g., 20) from the BOW_array.\n",
    "    \n",
    "    # Develops the Plot for records per Cluster\n",
    "    cluster_count_figure = sns.countplot(x='cluster_label_rev', data = df_data_filtered)\n",
    "    cluster_count_figure.set(ylim=(0, max(df_data_filtered['cluster_label_rev'].value_counts())*1.1)) # Increases the y-axis limit by 10% to ensure the annotation is inside of the border.\n",
    "    plt.xlabel(\"Cluster Label\", fontsize=14)\n",
    "    plt.ylabel(\"No. of Records\", fontsize=14)\n",
    "        \n",
    "    for p in cluster_count_figure.patches:\n",
    "        cluster_count_figure.annotate(format(p.get_height()), \n",
    "                                      (p.get_x()+p.get_width()/2, p.get_height()), \n",
    "                                      ha = 'center', \n",
    "                                      va = 'center', \n",
    "                                      xytext = (0, 10),\n",
    "                                      textcoords = 'offset points')\n",
    " \n",
    "    if df_data_filtered['cluster_label_rev'].min() == 0:\n",
    "        plotWords(df_best_features = df_best_features, top_n_terms = 10, elements_pie_chart = True, adjust_label = False)\n",
    "    if df_data_filtered['cluster_label_rev'].min() == 1:\n",
    "        plotWords(df_best_features = df_best_features, top_n_terms = 10, elements_pie_chart = True, adjust_label = True)\n",
    "    if (df_data_filtered['cluster_label_rev'].min() != 0) & (df_data_filtered['cluster_label_rev'].min() != 1):\n",
    "        print('Error with the Cluster Numbers. Please Check Clustering.')\n",
    "    cluster_plot_projection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_plot_projection():\n",
    "    # Cluster 2-DImension Plot Visualization. Can be used to verify clustering algorithm behavior.\n",
    "    if (text_model != 'None') & (text_model != 'Topic Model: LDA') & (dimensionality_reduction == 'PCA'):\n",
    "        cluster_plot_projection_w_name(plot_name = dimensionality_reduction)\n",
    "    if (text_model != 'None') & (text_model != 'Topic Model: LDA') & (dimensionality_reduction == 'TSNE'):\n",
    "        cluster_plot_projection_w_name(plot_name = dimensionality_reduction)\n",
    "    \n",
    "    if (text_model != 'None') & (text_model != 'Topic Model: LDA') & (cluster_two_D_plot != 'None'):\n",
    "        if (cluster_two_D_plot == 'PCA') & (dimensionality_reduction != 'PCA'):\n",
    "            PCA_func()\n",
    "            cluster_plot_projection_w_name(plot_name = cluster_two_D_plot)\n",
    "        if (cluster_two_D_plot == 'TSNE') & (dimensionality_reduction != 'TSNE'):\n",
    "            TSNE_func()\n",
    "            cluster_plot_projection_w_name(plot_name = cluster_two_D_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_plot_projection_w_name(plot_name):\n",
    "        # Projection Plot (non-scaled) of records in data. # Uncomment to show the non-scaled plot.\n",
    "        #display(HTML(f'<h2>{cluster_two_D_plot} Scatter Plot Projection<h2>'))\n",
    "        #plt.figure(figsize=(6,6))\n",
    "        #plt.scatter(bow_calc_array[:, 0], bow_calc_array[:, 1], marker='.', s=100, lw=0, alpha=1, c=None, edgecolor=None)\n",
    "        #plt.title(label = f\"PCA Plot with Data Reduced to {2} Components\")\n",
    "        #plt.show();\n",
    "\n",
    "        # Projection Plot (scaled) of records in data.\n",
    "        display(HTML(f'<h3>Scaled {plot_name} Scatter Plot Projection<h3>'))\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.scatter(bow_calc_array_scaled[:, 0], bow_calc_array_scaled[:, 1], marker='.', s=100, lw=0, alpha=1, c=None, edgecolor=None)\n",
    "        plt.title(label = f\"Scaled {plot_name} Plot with Data Reduced to {2} Components\")\n",
    "        plt.show();\n",
    "        \n",
    "        # Projection Plot (scaled) of records in data and assigned clusters.\n",
    "        display(HTML(f'<h3>Scaled {plot_name} Scatter Plot Projection with Assigned Clusters<h3>'))\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.scatter(bow_calc_array_scaled[:, 0], bow_calc_array_scaled[:, 1], marker='.', s=100, lw=0, alpha=1, c=df_data_filtered[\"cluster_label_rev\"].to_numpy(), edgecolor=None)\n",
    "        plt.title(label = f\"Clusters {plot_name} Scatter Plot Projection. Total Clusters = {total_clusters}\")\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "The optimal value of EPS is at the inflection point of the \"EPS (Distances) vs. Samples\" and is being calculated automatically as the closest or furthest point of the inflection to the corner of the plot when optimal value is used. \n",
    "\n",
    "The code allows to specify percent of records to cluster whcih could allow identification of outliers (e.g., using the 99% or a high percent). Given the high dimensionality and variability of the data it might be challenging to find an optimal valuefor all cases.\n",
    "\n",
    "The code also allows to identify the max clusters with the optimal EPS value and lowest minimum samples in the cluster (i.e., 2)\n",
    "These represents all or the maximum number of clusters within the optimal EPS value in the dataset. Identifying all the clusters and their record counts can help confirm common records. At a minimum, using the resulting topic models from the clusters (which could be in the hundreds) with the top counts could be used to verify those most occuring topics or those that have many records within a cluster.  If a tratidional trending analysis identifies electrical safety records as the most common the top non-zero cluster should represent electrical safety topics. If that is not the case then the analyst should verify why other topics are being identified as common. At the lowest number of minimum samples (e.g., 2), the results represent the theoretical optimal maximum number of clusters.\n",
    "\n",
    "The code also allows to identify the most dense clusters with the optimal EPS value and highest minimum samples that results in clusters. The most dense cluster should represent the most common records within the dataset using DBSCAN. Currently, this can be calculated using the structured data. However, this approach can identify specific topics and insights that may not be captured at the Keyword level.\n",
    "\n",
    "References: <br>\n",
    "- https://towardsdatascience.com/how-to-use-dbscan-effectively-ed212c02e62\n",
    "- https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc\n",
    "- https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_func(): # Calculates the distances between data points and estimates the optimal EPS.\n",
    "    global df_eps_vs_samples, eps_optimal, distances_scaled, eps_scaled_min, eps_scaled_max, eps_scaled_step\n",
    "    neigh = NearestNeighbors(n_neighbors=2)\n",
    "    nbrs = neigh.fit(bow_calc_array_scaled)\n",
    "    distances, indices = nbrs.kneighbors(bow_calc_array_scaled)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1] # Distances represent the values for EPS.\n",
    "\n",
    "    # DataFrame for the \"Samples vs. EPS\", normalization/scaling and Optimal EPS.\n",
    "    # Scaling transform all values between [0-1].\n",
    "    # It should be the same as using the SkLearn MaxAbsScaler or MinMaxScaler with range 0, 1.\n",
    "    df_eps_vs_samples = pd.DataFrame(distances, columns=['EPS'])\n",
    "    df_eps_vs_samples['samples_index'] = df_eps_vs_samples.index\n",
    "    df_eps_vs_samples['samples_index_scaled'] = df_eps_vs_samples['samples_index']/max(df_eps_vs_samples['samples_index'])\n",
    "    df_eps_vs_samples['EPS_scaled'] = df_eps_vs_samples['EPS']/max(df_eps_vs_samples['EPS'])\n",
    "    df_eps_vs_samples['euclidean_distance_to_brpoint'] = \"\"\n",
    "       \n",
    "    print(\"Please Wait, Process is Slow.\")\n",
    "    # Calculating the Euclidean distance to the bottorm right point (brpoint) of the TSNE plot that will be used to calculate euclidian distance to find the point closest and determine the most optimal EPS.\n",
    "    for i in range(0, df_eps_vs_samples.shape[0], 1):\n",
    "        if (dimensionality_reduction == 'TSNE'):\n",
    "            #Progress Status. But clears all of the other output in that cell.\n",
    "            clear_output(wait=True)\n",
    "            print(f'Current progress:', np.round(100*i/df_eps_vs_samples.shape[0], 0), '%')\n",
    "        \n",
    "        # Using the bottom right point as the relative point for calculation of inflection point. Can also use the top left. Not that when no DM is used the inflection point changes and is taken into consideration below.\n",
    "        df_eps_vs_samples.at[i, 'euclidean_distance_to_brpoint'] = np.linalg.norm(np.array((df_eps_vs_samples.at[i, 'EPS_scaled'], df_eps_vs_samples.at[i, 'samples_index_scaled'])) - np.array((min(df_eps_vs_samples['EPS_scaled']), max(df_eps_vs_samples['samples_index_scaled']))))\n",
    "        #df_eps_vs_samples.at[i, 'euclidean_distance_to_tlpoint'] = np.linalg.norm(np.array((df_eps_vs_samples.at[i, 'EPS_scaled'], df_eps_vs_samples.at[i, 'samples_index_scaled'])) - np.array((max(df_eps_vs_samples['EPS_scaled']), min(df_eps_vs_samples['samples_index_norm']))))\n",
    "\n",
    "    distances_scaled = np.array(df_eps_vs_samples['EPS_scaled'])\n",
    "    \n",
    "    # Optimal EPS is the point of inflection of the plot (i.e., lowest distance between inflection point and bottom right point when DM is used and viceversa when no DM is used).\n",
    "    if dimensionality_reduction == 'None':\n",
    "        df_eps_vs_samples = df_eps_vs_samples.sort_values(by = ['euclidean_distance_to_brpoint'], ascending=False).reset_index()\n",
    "    else:\n",
    "        df_eps_vs_samples = df_eps_vs_samples.sort_values(by = ['euclidean_distance_to_brpoint'], ascending=True).reset_index()\n",
    "    eps_optimal = df_eps_vs_samples['EPS_scaled'][0] # Optimal scaled EPS\n",
    "    \n",
    "    eps_scaled_min = min(df_eps_vs_samples['EPS_scaled']) # Min value of EPS scaled\n",
    "    eps_scaled_max = max(df_eps_vs_samples['EPS_scaled']) # Max value of EPS scaled\n",
    "    eps_scaled_step = (max(df_eps_vs_samples['EPS_scaled'])/100) # The step is 1% of the max EPS scaled\n",
    "    \n",
    "    input_data_stats_desc() # The Current Progress removes the input data stats description and hence added here again.\n",
    "    \n",
    "    # Plots for Samples Distance vs. EPS (scaled) and (non-scale)\n",
    "    plot1 = plt.figure(1, figsize=(6,6))\n",
    "    plt.title(label = f\"EPS vs. Samples (scaled)\")\n",
    "    plt.scatter(df_eps_vs_samples['samples_index_scaled'], df_eps_vs_samples['EPS_scaled'], marker='.', s=30, lw=0, alpha=1, c=None, edgecolor=None)    \n",
    "    plt.show();\n",
    "    \n",
    "    #plot2 = plt.figure(2, figsize=(6,6))\n",
    "    #plt.title(label = f\"EPS vs. Samples (non-scaled)\")\n",
    "    #plt.plot(distances);\n",
    "    #plt.show();\n",
    "    \n",
    "\n",
    "def dbscan_clusters(eps_1, min_samples): # Performs DBSCAN Clustering. \n",
    "    global cluster_pred, clusters, yhat, df_cluster_label, cluster_percent, total_clusters\n",
    "\n",
    "    # Define the DBSCAN model.  EPS is the distance between points, and represents a density value between data points.\n",
    "    model = DBSCAN(eps= eps_1, min_samples = min_samples)\n",
    "    # Fit model and predict clusters\n",
    "    yhat = model.fit_predict(bow_calc_array_scaled)\n",
    "    # retrieve unique clusters\n",
    "    clusters = unique(yhat)\n",
    "    \n",
    "    # Saving Model Labels to a variable\n",
    "    cluster_pred = model.labels_\n",
    "    \n",
    "    df_cluster_label = pd.DataFrame(cluster_pred, columns=['cluster_label_by_function']) # Model.labels_ gives an array and converting the array to a dataframe.\n",
    "    df_cluster_label['cluster_label_by_function'] += 1 # This modifies cluster \"-1\" as cluster \"0\" and all other clusters up.  This modification corrects an issue in the word plotting function where -1 is not plotting.\n",
    "    df_data_filtered['cluster_label_by_function'] = df_cluster_label # Adding column with cluster_labels to the full dataset dataframe.    \n",
    "    \n",
    "    cluster_percent = round(100*(df_cluster_label['cluster_label_by_function'].astype(bool).sum(axis=0))/(df_cluster_label.shape[0]),1) # Percent of non-zero cluster \n",
    "    total_clusters = df_cluster_label['cluster_label_by_function'].unique().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering (% Records to Cluster) Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "In the DBSCAN Clustering, Percent of Records to cluster, the user selects how many number of records to put in a non-zero cluster. The functions then iterate thru the EPS values to find the EPS where the selected percentage is met. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_cluster_by_percent_widget():\n",
    "    interact_manual(dbscan_cluster_by_percent, \n",
    "                    dbscan_percent_tocluster_W = widgets.IntSlider(min=1, \n",
    "                                                       max=100, \n",
    "                                                       value=95, \n",
    "                                                       step=1,\n",
    "                                                       description=\"% of Records to Cluster\",\n",
    "                                                       disables=False, style={'description_width': 'initial'},\n",
    "                                                       layout=widgets.Layout(width='50%')),\n",
    "                    min_samples_inacluster_W = widgets.IntSlider(min=1, \n",
    "                                                       max = 500, \n",
    "                                                       value = recommended_min_samples_inacluster,\n",
    "                                                       step = 1,\n",
    "                                                       description = \"Min. Samples in a Cluster\",\n",
    "                                                       disables = False, style={'description_width': 'initial'},\n",
    "                                                       layout = widgets.Layout(width='50%')),\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_cluster_by_percent(dbscan_percent_tocluster_W, min_samples_inacluster_W):\n",
    "    global dbscan_percent_tocluster, min_samples_inacluster, eps_iter, eps_iter_max, cluster_percent, total_clusters\n",
    "    \n",
    "    dbscan_percent_tocluster = dbscan_percent_tocluster_W\n",
    "    min_samples_inacluster = min_samples_inacluster_W\n",
    "    \n",
    "    # Since EPS are scaled it goes from 0 to 1. \n",
    "    eps_iter = 0 \n",
    "    eps_iter_max = 1\n",
    "    # DBSCAN is really slow when not using dimensionality reduction hence step is increased to 0.1. May not give good results\n",
    "    if dimensionality_reduction == 'None':\n",
    "        counter_min = 0.1\n",
    "    else:\n",
    "        counter_min = 0.01\n",
    "\n",
    "    while eps_iter < eps_iter_max:\n",
    "        #Progress Status\n",
    "        clear_output(wait=True)\n",
    "        print(f'Current progress:', np.round(100*eps_iter/eps_iter_max, 0), '%')\n",
    "        \n",
    "        # Iteration of EPS and Clustered Reports.\n",
    "        eps_iter += counter_min\n",
    "        dbscan_clusters(eps_iter, min_samples_inacluster)\n",
    "        \n",
    "        if dbscan_percent_tocluster < cluster_percent:  # Breaks if the specified percent to cluster threshold is met.\n",
    "            break;\n",
    "        \n",
    "    # Percent of reports not assigned to a cluster.\n",
    "    print(f'Results:\\n')\n",
    "    print(f'(1) A minimum cluster size of {min_samples_inacluster}, results in {cluster_percent}% of the {df_data_filtered.shape[0]} records being identified as non-zero cluster.  {100-cluster_percent}% of records are outliers (Cluster #0).\\n')\n",
    "    print(f'(2) The scaled EPS value used by the algorithm is {round(eps_iter, 2)}. There are a total of {total_clusters} (not including outliers).\\n')\n",
    "    print(f'Note: The optimal scaled EPS value for this dataset is {round(eps_optimal, 2)}. The optimal EPS value is not used in this case and is for information purposes only.')\n",
    "    \n",
    "    cluster_plots() # Plots for term scores and eLements and trending of records within the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering (Max Clusters w/Optimal EPS) Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "In the DBSCAN Clustering, Max Clusters with Optimal EPS, the user selects the minimum sample where the algorithm starts and then iterates from that value up to 500 using the optimal EPS and gives the user the number of min sample in a cluster that has the highest number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_max_clusters_opt_eps_widget():\n",
    "    interact_manual(dbscan_max_clusters_opt_eps, \n",
    "                    min_samples_inacluster_W = widgets.IntSlider(min=1, \n",
    "                                                       max = 500, \n",
    "                                                       value = 2,\n",
    "                                                       step = 1,\n",
    "                                                       description = \"Min. Samples in a Cluster\",\n",
    "                                                       disables = False, style={'description_width': 'initial'},\n",
    "                                                       layout = widgets.Layout(width='50%')),\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_max_clusters_opt_eps(min_samples_inacluster_W):\n",
    "    global min_samples_inacluster\n",
    "    \n",
    "    min_samples_inacluster = min_samples_inacluster_W\n",
    "    min_samples_vs_percent = []\n",
    "        \n",
    "    dbscan_clusters(eps_1 = eps_optimal, min_samples = min_samples_inacluster)\n",
    "    cluster_plots()\n",
    "    \n",
    "    row = [min_samples_inacluster, cluster_percent, df_cluster_label['cluster_label_by_function'].unique().max()]\n",
    "    min_samples_vs_percent.append(row) \n",
    "    # If you want to see the dataframe of min_samples in a cluster vs percent add to global\n",
    "\n",
    "    print(f'The results represent the number of clusters at the optimal EPS of {round(eps_optimal, 2)} and the minimum samples of {int(min_samples_inacluster)}. At the lowest number of minimum samples (e.g., 2), the results represent the theoretical optimal maximum number of clusters.\\n')\n",
    "    print(f'There are a total of {total_clusters} clusters with {cluster_percent}% of records. Outlier reports (i.e., Cluster #0) include {round(100-cluster_percent,2)}% of records.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering (Most Dense Cluster w/Optimal EPS) Widget and Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "In the DBSCAN Clustering, Most dense cluster with Optimal EPS, the algorithm finds the value where there are the most records in one cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_dense_clusters_opt_eps_widget():\n",
    "    interact_manual(dbscan_dense_cluster_opt_eps, \n",
    "                    max_iter_samples_inacluster_W = widgets.IntSlider(min=1, \n",
    "                                                                   max = 500, \n",
    "                                                                   value = 500,\n",
    "                                                                   step = 1,\n",
    "                                                                   description = \"Min. Samples in a Cluster\",\n",
    "                                                                   disables = False, style={'description_width': 'initial'},\n",
    "                                                                   layout = widgets.Layout(width='50%')),\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_dense_cluster_opt_eps(max_iter_samples_inacluster_W):\n",
    "    global min_samples_inacluster\n",
    "    # Max Clusters Using the Optimal EPS value.\n",
    "    min_samples_inacluster = 1 # Iteration starts at 1 up to the max.\n",
    "    min_samples_vs_percent = []\n",
    "\n",
    "    while min_samples_inacluster < max_iter_samples_inacluster_W: # Iterates up to a max of max_iter_samples_inacluster_W.\n",
    "        #Progress Status\n",
    "        clear_output(wait=True)\n",
    "        print(f'Current progress:', np.round(100*min_samples_inacluster/max_iter_samples_inacluster_W, 0), '%')\n",
    "        \n",
    "        row = []\n",
    "        min_samples_inacluster += 1\n",
    "        dbscan_clusters(eps_1 = eps_optimal, min_samples = min_samples_inacluster)\n",
    "    \n",
    "        row = [min_samples_inacluster, cluster_percent, df_cluster_label['cluster_label_by_function'].unique().max()]\n",
    "        min_samples_vs_percent.append(row)\n",
    "    \n",
    "        if cluster_percent == 0:  # Breaks when cluster percent equals 0.\n",
    "            break;\n",
    "\n",
    "    min_samples_inacluster -= 1 # This gives the last min_sample case and results in the most reports assigned a cluster.  This highlights high concentration of reports.\n",
    "\n",
    "    dbscan_clusters(eps_1 = eps_optimal, min_samples = min_samples_inacluster)\n",
    "    cluster_plots()\n",
    "    \n",
    "    row = [min_samples_inacluster, cluster_percent, df_cluster_label['cluster_label_by_function'].unique().max()]\n",
    "    min_samples_vs_percent.append(row)\n",
    "    \n",
    "    print(f'The results represent the most dense cluster at the optimal EPS of {round(eps_optimal, 3)} and the highest minimum samples that results in a cluster(s) {int(min_samples_inacluster)}.')\n",
    "    print(f'There are a total of {total_clusters} clusters with {cluster_percent}% of records. Outlier reports (i.e., Cluster #0) include {round(100-cluster_percent,2)}% of records.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering (Custom EPS and Min Samples in a Cluster) Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "In the DBSCAN Custom EPS and Min Sampels in a Cluster the user can select whatever EPS and Minimum samples in a cluster they want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_cluster_custom_widget():\n",
    "    interact_manual(dbscan_cluster_custom, \n",
    "                    eps_custom_W = widgets.FloatSlider(min=eps_scaled_min, \n",
    "                                                     max=eps_scaled_max, \n",
    "                                                     value=eps_optimal, \n",
    "                                                     step=eps_scaled_step,\n",
    "                                                     description=\"Scaled EPS\",\n",
    "                                                     disables=False, style={'description_width': 'initial'},\n",
    "                                                     layout=widgets.Layout(width='50%')),\n",
    "                    min_samples_inacluster_custom_W = widgets.IntSlider(min=1, \n",
    "                                                                      max = 500, \n",
    "                                                                      value = recommended_min_samples_inacluster,\n",
    "                                                                      step = 1,\n",
    "                                                                      description = \"Min. Samples in a Cluster\",\n",
    "                                                                      disables = False, style={'description_width': 'initial'},\n",
    "                                                                      layout = widgets.Layout(width='50%')),\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_cluster_custom(eps_custom_W, min_samples_inacluster_custom_W):\n",
    "    global eps_custom, min_samples_inacluster_custom\n",
    "    eps_custom = eps_custom_W\n",
    "    min_samples_inacluster_custom = min_samples_inacluster_custom_W\n",
    "    \n",
    "    dbscan_clusters(eps_1 = eps_custom, min_samples = min_samples_inacluster_custom)\n",
    "    \n",
    "    # Percent of reports not assigned to a cluster.\n",
    "    print(f'Results:\\n')\n",
    "    print(f'(1) A scaled EPS of {round(eps_custom, 2)} and minimum cluster size of {min_samples_inacluster_custom}, results in a total of {total_clusters} (not including outlier Cluster#0) and {cluster_percent}% of the {df_data_filtered.shape[0]} records being identified as non-zero cluster.  {100-cluster_percent}% of records are identified as outliers (Cluster #0).\\n')\n",
    "    print(f'(2) The optimal scaled EPS value is {round(eps_optimal, 2)} and the recommended minimum sample sin a cluster is {recommended_min_samples_inacluster}.\\n')\n",
    "    print(f'Note: The optimal scaled EPS value for this dataset is {round(eps_optimal, 2)}. The optimal EPS value is not used in this case and is for information purposes only.')\n",
    "    \n",
    "    cluster_plots() # Plots for term scores and elements and trending of records within the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "References: <br>\n",
    "- https://medium.com/@lucasdesa/text-clustering-with-k-means-a039d84a941b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_optimal_k_widget():\n",
    "    interact(kmeans_optimal_k, \n",
    "             kmeans_optimal_k_W = widgets.Dropdown(options= ['Silhoutte Score', 'KMeans Distoritions', 'KMeans Score'], \n",
    "                                                   description = 'Optimal K Method',\n",
    "                                                   value = 'Silhoutte Score',\n",
    "                                                   disabled=False, style={'description_width': 'initial'},\n",
    "                                                   layout=widgets.Layout(width='40%')),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_optimal_k(kmeans_optimal_k_W):\n",
    "    # This fixes the error when there are less than 50 records and adjusts the K to the number of records.\n",
    "    if df_data_filtered.shape[0] < 50:\n",
    "        K = range(2, df_data_filtered.shape[0])\n",
    "    if df_data_filtered.shape[0] >=50:\n",
    "        K = range(2, 50)\n",
    "    km_scores= []\n",
    "    km_silhouette = []\n",
    "    distortions = []\n",
    "    \n",
    "    for i in K:\n",
    "        km = KMeans(n_clusters=i, max_iter=1000, n_init=10, random_state=0).fit(bow_calc_array_scaled)\n",
    "        preds = km.predict(bow_calc_array_scaled)\n",
    "        km_scores.append(-km.score(bow_calc_array_scaled))\n",
    "        km.fit(bow_calc_array_scaled)\n",
    "        distortions.append(sum(np.min(cdist(bow_calc_array_scaled, km.cluster_centers_, 'euclidean'), axis=1)) / bow_calc_array_scaled.shape[0])\n",
    "        silhouette = silhouette_score(bow_calc_array_scaled, preds)\n",
    "        km_silhouette.append(silhouette)\n",
    "\n",
    "    if kmeans_optimal_k_W == 'Silhoutte Score':\n",
    "        plt.figure(figsize=(14,4))\n",
    "        plt.title(\"The silhouette coefficient method \\nfor determining the optimal number of clusters\\n\",fontsize=16)\n",
    "        plt.scatter(x=[i for i in K],y=km_silhouette,s=150,edgecolor='k')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"Number of clusters\",fontsize=12)\n",
    "        plt.ylabel(\"Silhouette score\",fontsize=15)\n",
    "        plt.xticks([i for i in K],fontsize=10)\n",
    "        plt.yticks(fontsize=12)\n",
    "        print('Silhouette score: measure of how similar an object is to its own cluster compared to other clusters. Values range from −1 to +1. A high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. Value of 1 means that the clusters are very dense and nicely separated. A score of 0 means that clusters are overlapping. Score less than 0 means that data belonging to clusters may be wrong/incorrect.')\n",
    "        print('References: \"Silhouette (clustering)\" at Wikipedia and \"KMeans Silhouette Score Explained With Python Example\" Published at DZone with permission of Ajitesh Kumar.')\n",
    "        plt.show();\n",
    "        \n",
    "    if kmeans_optimal_k_W == 'KMeans Distoritions':# Plot the elbow using Kmeans Distortions\n",
    "        plt.figure(figsize=(14,4))\n",
    "        plt.title(\"The elbow method for determining number of clusters\\n\",fontsize=16)\n",
    "        plt.scatter(K, distortions, s=150,edgecolor='k')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"Number of clusters\", fontsize=14)\n",
    "        plt.ylabel(\"K-means Distortion\", fontsize=15)\n",
    "        plt.xticks(K,fontsize=12)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.show();\n",
    "        \n",
    "    if kmeans_optimal_k_W == 'KMeans Score':# Plot the elbow using Kmeans Score\n",
    "        plt.figure(figsize=(14,4))\n",
    "        plt.title(\"The elbow method for determining number of clusters\\n\",fontsize=16)\n",
    "        plt.scatter(x=[i for i in K],y=km_scores,s=150,edgecolor='k')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"Number of clusters\",fontsize=14)\n",
    "        plt.ylabel(\"K-means score\",fontsize=15)\n",
    "        plt.xticks([i for i in K],fontsize=10)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_k_clusters_widget():\n",
    "    interact_manual(kmeans_k_clusters, \n",
    "                    total_clusters_W = widgets.IntSlider(min=2, \n",
    "                                                   max=50, \n",
    "                                                   value=3, \n",
    "                                                   step=1,\n",
    "                                                   description=\"KMEANS No. Clusters\",\n",
    "                                                   disables=False, style={'description_width': 'initial'},\n",
    "                                                   layout=widgets.Layout(width='50%')\n",
    "                                                  )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_k_clusters(total_clusters_W):\n",
    "    global cluster_pred, total_clusters\n",
    "    total_clusters = total_clusters_W\n",
    "    \n",
    "    if total_clusters_W > len(df_data_filtered):\n",
    "        print(f'Please select a lower number of clusters. There are more clusters ({total_clusters_W}) than records ({len(df_data_filtered)}).')\n",
    "    else:\n",
    "        print(f'{total_clusters_W} Clusters were selected.')\n",
    "        # Kmeans function to assign predicted cluster for record.\n",
    "        cluster_pred = KMeans(n_clusters=total_clusters_W, random_state=42, max_iter=1000, n_init=10).fit_predict(bow_calc_array_scaled)\n",
    "\n",
    "        df_cluster_label = pd.DataFrame(cluster_pred, columns=['cluster_label_by_function']) # Model.labels_ gives an array and converting the array to a dataframe.\n",
    "        df_cluster_label['cluster_label_by_function'] += 1 # This modifies cluster \"0\" as cluster \"1\" and all other clusters up.  This modification clarifies that there is no Cluster 0.\n",
    "    \n",
    "        # Percent of non-zero cluster.\n",
    "        cluster_percent = round(100*(df_cluster_label['cluster_label_by_function'].astype(bool).sum(axis=0))/(df_cluster_label.shape[0]),1) \n",
    "        print(f'{cluster_percent}% of records assigned to a cluster.')\n",
    "    \n",
    "        df_data_filtered['cluster_label_by_function'] = df_cluster_label # Adding column with cluster_labels to the full dataset dataframe.    \n",
    "    \n",
    "        # WOULD BE NICE TO CALCULATE/SORT THE CLUSTER NUMBER BASED ON AVERAGE TOP 10 TERM SCORES WHERE CLUSTER 1 HAD THE HIGHEST AND DESCENDING FROM THERE.\n",
    "        # WOULD BE NICE TO CALCULATE/SORT THE CLUSTER NUMBER BASED ON AVERAGE TOP 10 TERM SCORES WHERE CLUSTER 1 HAD THE HIGHEST AND DESCENDING FROM THERE.\n",
    "        # WOULD BE NICE TO CALCULATE/SORT THE CLUSTER NUMBER BASED ON AVERAGE TOP 10 TERM SCORES WHERE CLUSTER 1 HAD THE HIGHEST AND DESCENDING FROM THERE.\n",
    "    \n",
    "        cluster_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "###TBD TBD NEED TO IMPLEMENT IF NEEDED. This is for Selection of the OPTIMAL K CLUSTERS in a similar Fashion to MICROSTRATEGY.\n",
    "\n",
    "def kmeans_optimal_k_clusters(total_clusters_W, K_clusters_limit):\n",
    "    # Similar to how MicroStrategy defines it puts an upper limit to the total_clusters_W. Could use the Elbow or the Silhoutte best value and just run Kmeans with the Optimal value.\n",
    "    print('Running iwth the Optimal Number needs to be setup. Manual optimal K can be manually selected using the Kmeans w/Optimal K.')\n",
    "    print(total_clusters_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Top Terms WordCloud Widget and Clustering Plots Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "The top terms widget has the functions for developing the the word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features_cluster(bow_array, prediction, n_feats):\n",
    "    global df_best_features, df_label_by_ftrs_score, df_best_features_raw\n",
    "    # Note that the Clustering Functions assigns the clustering labels in somewhat random fashion (cluster_label_by_function). This function here revises the non-zero cluster labels to be sorted by mean of the top 10 term scores.\n",
    "    \n",
    "    labels = np.unique(prediction)\n",
    "    df_best_features_raw = []\n",
    "    df_label_by_ftrs_score = pd.DataFrame()\n",
    "\n",
    "    for label in labels:\n",
    "        id_temp = np.where(prediction==label) # Temporary ID for each record within the cluster cluster\n",
    "        x_means = np.mean(bow_array[id_temp], axis = 0) # Returns average score across cluster\n",
    "        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top n_feats scores\n",
    "        best_features = [(features[i], x_means[i]) for i in sorted_means] # Variable \"features\" is created in the bow_vectorizer_data function.\n",
    "        df = pd.DataFrame(best_features, columns = ['Terms', 'Avg. Score']) # Dataframes with the Terms and Avg. Scores \n",
    "        df_best_features_raw.append(df) # List of Dataframes with the Terms and Avg. Scores\n",
    "\n",
    "        # Calculates the top 10 terms average score for each Cluster Label.\n",
    "        # Note that the \"label-1\" is the index of the label in the df_best_features_raw list of dataframes and needs to be used for clustering methods that don't produce a cluster number 0.  \n",
    "        if min(labels) == 0: # Used by the Top Terms and DBSCAN whcih both have zero assigned to clusters.\n",
    "            df2 = pd.DataFrame(data = [[label, round(mean(df_best_features_raw[label][:10]['Avg. Score']),4), int(0)]], \n",
    "                               columns = ['cluster_label_by_function', \n",
    "                                          'top_10_terms_avg_score', 'cluster_label_rev'])\n",
    "        if min(labels) > 0: # Used by KMeans or DBSCAN when there is no cluster zero.\n",
    "            df2 = pd.DataFrame(data = [[label, round(mean(df_best_features_raw[label-1][:10]['Avg. Score']),4), int(0)]], \n",
    "                               columns = ['cluster_label_by_function', \n",
    "                                          'top_10_terms_avg_score', 'cluster_label_rev'])  \n",
    "        #df_label_by_ftrs_score = df_label_by_ftrs_score.append(df2) # Will be depracated\n",
    "        df_label_by_ftrs_score = pd.concat([df_label_by_ftrs_score, df2], ignore_index = True)\n",
    "        \n",
    "    # Checks if there is a cluster 0 and if there is removes the row as df_temp as we want to keep the cluster 0 which represents terms not assigned a cluster.\n",
    "    df_temp = pd.DataFrame()\n",
    "    if (min(df_label_by_ftrs_score['cluster_label_by_function']) == 0):\n",
    "        df_temp = df_label_by_ftrs_score.loc[(df_label_by_ftrs_score['cluster_label_by_function'] == 0)].copy()\n",
    "        df_label_by_ftrs_score = df_label_by_ftrs_score[df_label_by_ftrs_score['cluster_label_by_function'] != 0]\n",
    "    \n",
    "    # Sorts the non-zero clusters by the top_10_terms_avg_score\n",
    "    df_label_by_ftrs_score = df_label_by_ftrs_score.sort_values(by='top_10_terms_avg_score', ascending=False)\n",
    "    # Once sorted, creates a revised clsuter label where the cluster with the highest avg score is first and descending from there.\n",
    "    df_label_by_ftrs_score.cluster_label_rev = range(1, 1 + len(df_label_by_ftrs_score))\n",
    "\n",
    "    # Reinserts cluster 0 to the final dataframe were clusters have been sorted by top 10 average feature score\n",
    "    if not df_temp.empty:\n",
    "        df_label_by_ftrs_score = pd.concat([df_temp, df_label_by_ftrs_score], axis=0)\n",
    "    \n",
    "    # Final Dataframe with the crosswalk of the revised label sorted by the top 10 average score.\n",
    "    df_label_by_ftrs_score = df_label_by_ftrs_score.reset_index(drop = True) # Resets index.\n",
    "    \n",
    "    # Adds the revised cluster label to the df_data_filtered\n",
    "    df_data_filtered['top_10_terms_avg_score'] = np.nan # Adds new column to the filtered data and sets values to nan.\n",
    "    df_data_filtered['cluster_label_rev'] = 0 # Adds new column to the filtered data and sets values to 0.\n",
    "    for row in range(len(df_data_filtered)): # Iterates thru the original cluster_labels and modifies them to those sorted by top 10 term average score.\n",
    "        # Uses the crosswalk from df_label_by_ftrs_score to find the cluster label top_10_terms_avg_score.\n",
    "        df_data_filtered.at[row, 'top_10_terms_avg_score'] = round(df_label_by_ftrs_score.loc[df_label_by_ftrs_score['cluster_label_by_function'] == df_data_filtered.at[row, 'cluster_label_by_function'], 'top_10_terms_avg_score'], 4)\n",
    "        # Uses the crosswalk from df_label_by_ftrs_score to find the revised cluster label.\n",
    "        df_data_filtered.at[row, 'cluster_label_rev'] = df_label_by_ftrs_score.loc[df_label_by_ftrs_score['cluster_label_by_function'] == df_data_filtered.at[row, 'cluster_label_by_function'], 'cluster_label_rev'].astype(int)\n",
    "    \n",
    "    if min(labels) == 0: # Used by the Top Terms and DBSCAN whcih both have zero assigned to clusters.\n",
    "        rev_order = (df_label_by_ftrs_score['cluster_label_by_function']-1).tolist()\n",
    "    if min(labels) > 0: # Used by KMeans or DBSCAN when there is no cluster zero.\n",
    "        rev_order = (df_label_by_ftrs_score['cluster_label_by_function']-1).tolist()\n",
    "    df_best_features = [df_best_features_raw[i] for i in rev_order] # Sorts the list of dataframes based on the mean of the top 10 terms average score and keeping the cluster 0 as 0. \n",
    "    \n",
    "    return df_best_features\n",
    " \n",
    "        \n",
    "def plotWords(df_best_features, top_n_terms, elements_pie_chart, adjust_label):\n",
    "    fig_height = top_n_terms/2.5\n",
    "    for label in range(0, len(df_best_features)):\n",
    "        # Note that the \"df_best_features\" data frame resets its index to 0 hence the adjusting of the label+1 which is only needed for Kmeans or when DBSCAN finds no Cluster 0.   \n",
    "        if adjust_label == False:\n",
    "            cluster_label = label\n",
    "        if adjust_label == True:\n",
    "            cluster_label = label+1 # Adjusting the Cluster Label list to the df_to_cluster labels.  \n",
    "\n",
    "        f, ax = plt.subplots(figsize=(6, fig_height))\n",
    "        plt.title((f\"Most Common Terms in Cluster #{cluster_label}\"), fontsize=10, fontweight='bold')\n",
    "        sns.barplot(x = 'Avg. Score', y = 'Terms', data = df_best_features[label][:top_n_terms], palette = ['#006BA4'])\n",
    "        ax.set(xlim=(0, max(df_best_features[label][:top_n_terms]['Avg. Score'])*1.1)) # Increases the x-axis limit by 10% to ensure the annotation is inside of the border.\n",
    "        \n",
    "        for p in ax.patches:\n",
    "            width = p.get_width()    # get bar length\n",
    "            ax.text(width,       # If 'width' with no quotes, sets the text to start right of the bar\n",
    "                    p.get_y() + p.get_height()/2, # get Y coordinate + X coordinate / 2\n",
    "                    '{:.3f}'.format(width), # set variable to display, 2 decimals\n",
    "                    ha = 'left',   # horizontal alignment\n",
    "                    va = 'center')  # vertical alignment\n",
    "        plt.show();\n",
    "        \n",
    "        if elements_pie_chart == True:\n",
    "            # Pie Chart of original_language\n",
    "            show_threshold = 3\n",
    "            # If the count for top original_language is larger than the threshold will show the plot. First steps checks that the number of records is more than 0. Second step shows the plot with only Categories that have counts more than the threshold.\n",
    "            if len(df_data_filtered[(df_data_filtered['cluster_label_rev'] == cluster_label)].original_language.value_counts(dropna = False).loc[lambda x : x>show_threshold]) != 0:\n",
    "                if df_data_filtered[(df_data_filtered['cluster_label_rev'] == cluster_label)].original_language.value_counts(dropna = False).loc[lambda x : x>show_threshold][0] > show_threshold:\n",
    "                    plt.subplots(figsize=(6, fig_height))\n",
    "                    df_data_filtered[(df_data_filtered['cluster_label_rev'] == cluster_label)].original_language.value_counts(dropna = False).loc[lambda x : x>show_threshold].plot.pie()\n",
    "                    plt.title(f\"Original Languages for Cluster #{cluster_label}\",fontsize=16)\n",
    "                    plt.xlabel(f\"Original Languages (*> {show_threshold} Records)\",fontsize=12)\n",
    "                    plt.ylabel(\"\",fontsize=15)\n",
    "                    plt.show();\n",
    "            else:\n",
    "                print(f'PIE CHART NOT SHOWN: Cluster #{cluster_label} does not meet threshold of at least {show_threshold} records per Original Language.')\n",
    "            \n",
    "        # Prints the trend plot if a timeperiod is selected (!=None) and there are at least two time periods.\n",
    "        if (trend_timeperiod != 'None'):\n",
    "            if (len(df_data_filtered[(df_data_filtered[trend_timeperiod] != 'NaT')][trend_timeperiod].value_counts()) > 1):\n",
    "                plot_cluster_trend(input_data = df_data_filtered, cluster_label = cluster_label)\n",
    "\n",
    "\n",
    "def plot_cluster_trend(input_data, cluster_label): \n",
    "    global df_cluster_counts\n",
    "    # Dataframe of Cluster's Record Counts per Time.\n",
    "    df_cluster_counts = (input_data.loc[(input_data['cluster_label_rev'] == cluster_label)]\n",
    "                                 .copy()\n",
    "                                 .groupby([trend_timeperiod],as_index=False)\n",
    "                                 .agg({'cluster_label_rev': 'count'})) # Aggregating counting the items in the filtered dataframe by year.\n",
    "    df_cluster_counts = df_cluster_counts.rename(columns={\"cluster_label_rev\": \"record_counts\"})\n",
    "    \n",
    "    # Removes NaT/NaNs and convert to Float to Calculate correctly the trendline equation values.\n",
    "    df_cluster_counts = df_cluster_counts.loc[(df_cluster_counts[trend_timeperiod] != 'NaT') & \n",
    "                                              (df_cluster_counts[trend_timeperiod] != np.nan)]\n",
    "    df_cluster_counts[trend_timeperiod] = df_cluster_counts[trend_timeperiod].astype(float)\n",
    "\n",
    "    # Plots the data and trendline only when the data has more than 1 time period (e.g., year). Otherwise does not calculate the trend.\n",
    "    if (len(df_cluster_counts[trend_timeperiod].unique()) > 1):\n",
    "        print(f'Linear Trend of Records within Cluster #{cluster_label}')                                                           \n",
    "        bar_chart_wtrend(input_data = df_cluster_counts, x_feature = trend_timeperiod, \n",
    "                         y_feature='record_counts', add_trend ='Linear', add_equation = True)\n",
    "    else:\n",
    "        print(f'Linear Trend of Records within Cluster #{cluster_label}')                                                           \n",
    "        bar_chart_wtrend(input_data = df_cluster_counts, x_feature = trend_timeperiod, \n",
    "                         y_feature='record_counts', add_trend ='None', add_equation = False)\n",
    "    print('Note: Algorithm removes Null values for time period plots and calculating the trendline.')\n",
    "    plt.show(); \n",
    "    \n",
    "    \n",
    "def wordcloud_plot(n_feats):\n",
    "    Cloud = WordCloud(background_color=\"white\",\n",
    "                      max_words = n_feats,\n",
    "                      random_state = 49,\n",
    "                      normalize_plurals = False,\n",
    "                      colormap= 'tab20').generate_from_frequencies(df_bow.T.mean(axis=1))\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.imshow(Cloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show();   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Terms Wordcloud Widget\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_terms_widget():\n",
    "    interact_manual(top_terms, \n",
    "                    word_preprocessing_W = widgets.Dropdown(options= ['Lemmatization', 'Stemming'], \n",
    "                                                                       description = 'Word Preprocessing Method',\n",
    "                                                                       value = 'Lemmatization',\n",
    "                                                                       disabled=False, style={'description_width': 'initial'},\n",
    "                                                                       layout=widgets.Layout(width='40%')),\n",
    "                    vect_W = widgets.Dropdown(options= ['TFIDF', 'Count'], \n",
    "                                              description = 'Word Vectorizer',\n",
    "                                              value = 'TFIDF',\n",
    "                                              disabled=False, style={'description_width': 'initial'},\n",
    "                                              layout=widgets.Layout(width='40%')),\n",
    "                    n_terms_W = widgets.IntSlider(min=10, \n",
    "                                                  max=100, \n",
    "                                                  value=20, \n",
    "                                                  step=1,\n",
    "                                                  description=\"Number of Terms to Plot\",\n",
    "                                                  disables=False, style={'description_width': 'initial'},\n",
    "                                                  layout=widgets.Layout(width='50%')),\n",
    "                    max_df_W = widgets.FloatSlider(min=0.05, \n",
    "                                                   max=1.0, \n",
    "                                                   value=0.95, \n",
    "                                                   step=0.05,\n",
    "                                                   description=\"Term Maximum Document Frequency\",\n",
    "                                                   disables=False, style={'description_width': 'initial'},\n",
    "                                                   layout=widgets.Layout(width='50%'),\n",
    "                                                   readout_format='.2f'),\n",
    "                    min_df_W = widgets.FloatSlider(min=0.0001, \n",
    "                                                   max=0.1, \n",
    "                                                   value=0.01, \n",
    "                                                   step=0.005,\n",
    "                                                   description=\"Term Minimum Document Frequency\",\n",
    "                                                   disables=False, style={'description_width': 'initial'},\n",
    "                                                   layout=widgets.Layout(width='50%'),\n",
    "                                                   readout_format='.4f'),\n",
    "                    trend_timeperiod_W = widgets.Dropdown(options= ['None', 'release_cy'],\n",
    "                                                                  description = 'Cluster Trend by',\n",
    "                                                                  value = 'release_cy',\n",
    "                                                                  disabled=False, style={'description_width': 'initial'},\n",
    "                                                                  layout=widgets.Layout(width='40%')),\n",
    "                   )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_terms(word_preprocessing_W, vect_W, n_terms_W, max_df_W, min_df_W, trend_timeperiod_W):\n",
    "    global word_preprocessing, vect, n_terms, max_df, min_df, cluster_pred, df_best_features, trend_timeperiod\n",
    "    word_preprocessing = word_preprocessing_W\n",
    "    vect = vect_W\n",
    "    n_terms = n_terms_W\n",
    "    max_df = max_df_W\n",
    "    min_df = min_df_W\n",
    "    trend_timeperiod = trend_timeperiod_W\n",
    "    \n",
    "    df_data_filtered['cluster_label_by_function'] = 0 # Sets cluster label to 0 as initial value to extrat top topics accross the filtered data without running cluster functions.\n",
    "    cluster_pred = df_data_filtered[\"cluster_label_by_function\"].to_numpy() # Converting cluster_label assigments to an array.\n",
    "    \n",
    "    bow_vectorizer_data(input_data = df_data_filtered, vect = vect, word_preprocessing = word_preprocessing,\n",
    "                       max_df = max_df, min_df = min_df)\n",
    "    \n",
    "    df_best_features = get_top_features_cluster(bow_array = bow_array, prediction = cluster_pred, n_feats = n_terms)\n",
    "    wordcloud_plot(n_feats = n_terms)\n",
    "    plotWords(df_best_features = df_best_features, top_n_terms = n_terms, elements_pie_chart = False, adjust_label = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling Widget\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "This widget uses the linear regression and Poisson Distribution to predict value of future years.\n",
    "\n",
    "References: <br>\n",
    "- Box Plots: https://en.wikipedia.org/wiki/Box_plot\n",
    "- Kruskal-Wallis H statistic: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html\n",
    "- Scatter Plot with Confidence Intervals: https://stackoverflow.com/questions/27164114/show-confidence-limits-and-prediction-limits-in-scatter-plot\n",
    "- Poisson Distributions: https://www.statology.org/poisson-distribution-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive_widget():\n",
    "    interact_manual(predictive_models, \n",
    "                    predictive_model_W = widgets.Dropdown(options=['LR and Poisson', 'PLACEHOLDER: Monte Carlo'],\n",
    "                                                            value = 'LR and Poisson',\n",
    "                                                            description = 'Predictive Model', \n",
    "                                                            disabled=False, style={'description_width': 'initial'},\n",
    "                                                            layout=widgets.Layout(width='50%'))\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive_models(predictive_model_W):\n",
    "    if predictive_model_W == 'LR and Poisson':\n",
    "        lr_poisson_prediction_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_poisson_prediction_widget():\n",
    "    print('The Linear Regression and Poisson Distribution models use the filtered data to forecast the expected value of the following timeperiod (e.g., CY, FY).')\n",
    "    print('\\nFor example, if the user filters for 2016 to 2019 CY data and selects the release_cy, the model will show the predicted value for 2020 CY using a LR (red bar) and a Poisson Distribution (blue boxplot). Where data is available for that proceeding year it will show the occurrence data as a yellow bar.')\n",
    "    print('\\nThe Linear Regression equation for the selected data is provided in the summary table.')\n",
    "    print('\\nThe Poisson Distribution uses a parameter called Lambda (shown as a orange dotted line). This parameter is equal to the average count observed under the previous period. In this tool, the Lambda value can be set to the average of all selected years or fix it to the value of last year.')\n",
    "    \n",
    "    interact_manual(lr_poisson_prediction, \n",
    "                    x_feature = widgets.Dropdown(options=['release_cy'],\n",
    "                                                 value = 'release_cy',\n",
    "                                                 description = 'Time Period', \n",
    "                                                 disabled=False, style={'description_width': 'initial'},\n",
    "                                                 layout=widgets.Layout(width='50%')),\n",
    "                    lmbda_period_term = widgets.Dropdown(options=['Last Selected Period', 'Average of All Selected Periods'],\n",
    "                                                 value = 'Last Selected Period',\n",
    "                                                 description = 'Poisson Lambda Value', \n",
    "                                                 disabled=False, style={'description_width': 'initial'},\n",
    "                                                 layout=widgets.Layout(width='50%'))\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_poisson_prediction(x_feature, lmbda_period_term):\n",
    "    global input_data\n",
    "    # Value Counts of records per selected x_feature/timeframe (i.e., release_cy, DATE_FY, etc.).\n",
    "    input_data = df_data_filtered[x_feature].value_counts().rename_axis(x_feature).reset_index(name='counts').sort_values(by=[x_feature]) # input data source. Could use the already defined function. May want to clean the name out of per FY or define in the sameway a value_counts.\n",
    "    input_data = input_data.loc[(input_data[x_feature] != 'NaT') & \n",
    "                                (input_data[x_feature] != np.nan)].copy().reset_index(drop = True) # Removes NaN values.\n",
    "    input_data = input_data.apply(pd.to_numeric, errors='raise') # Converts the dataframe columns to numeric (recall the 'NaT' was string and is removed in previous step).\n",
    "\n",
    "    # Used to calculate the proceeding year data point. \n",
    "    # Does the same filtering as the main function without using the time related filters.\n",
    "    df_data_filtered_wo_dates = df_data.loc[(df_data['genres'].str.contains('|'.join(genres_winput), \n",
    "                                                                            flags=re.IGNORECASE, regex=True))\n",
    "                                            ].copy().reset_index(drop = True)\n",
    "    \n",
    "    # Count of x_features (e.g., release_cy) in the source data.  Used to find if the predicted year has data and show it.\n",
    "    df_data_counts = df_data_filtered_wo_dates[x_feature].value_counts().rename_axis(x_feature).reset_index(name='counts')\n",
    "    \n",
    "    # X and Y Data values\n",
    "    x_values = input_data[x_feature]\n",
    "    y_values = input_data['counts']\n",
    "    \n",
    "    # GENERIC INPUT PARAMETERS AND INDEX \n",
    "    end_period_term_index = input_data.shape[0] # Index for last year for mean and loop calculations.\n",
    "    timeperiod_predicted = max(x_values)+1 # Creates the variable for the time period being predicted (i.e., plus one the last period in selected range).\n",
    "    \n",
    "\n",
    "    # POISSON PARAMETERS AND \n",
    "    sample_size = 10000\n",
    "    if lmbda_period_term == 'Last Selected Period':\n",
    "        lmbda = int(mean(y_values[end_period_term_index-1:end_period_term_index])) # Mean of selected year data.\n",
    "    elif lmbda_period_term == 'Average of All Selected Periods':\n",
    "        lmbda = int(mean(y_values)) # Mean of selected year data.\n",
    "    \n",
    "    x_random = poisson.rvs(mu=lmbda, size=sample_size) # Poisson random samples. Used in Histogram.\n",
    "    x_random_sorted = np.sort(x_random) # Sorting the Poisson random sample values (i.e., occurrence). Used in CDF and df_CDF\n",
    "    cdf_y_values = np.arange(sample_size)/float(sample_size) # Sorting the values of the probabilities CDF values (i.e., values of y). Used in CDF and df_CDF.\n",
    "    df_CDF = pd.DataFrame({f'{timeperiod_predicted}':x_random_sorted, 'probability':cdf_y_values})\n",
    "\n",
    "    # LINEAR TREND PARAMETERS AND EQUATION\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(input_data[[x_feature]], y_values)\n",
    "    y_intercept_min_year = linreg.coef_[0]*x_values.min()+linreg.intercept_ # y-intercept value.\n",
    "    y_pred = linreg.predict(input_data[[x_feature]]) # y_predicted values.\n",
    "    \n",
    "    # STATISTIC FIT MEASURES AND METRICS\n",
    "    f_reg = f_regression(X = input_data[[x_feature]], y = y_values, center = True)\n",
    "    f_statistic = f_reg[0]\n",
    "    p_value = f_reg[1]\n",
    "    \n",
    "    # LINEAR REGRESSION METRICS USING SCIPY\n",
    "    slope_scipy, intercept_scipy, r_value_scipy, p_value_scipy, std_err_scipy = stats.linregress(x = x_values, y = y_values)\n",
    "    confidence_interval_scipy = 2*std_err_scipy # For a Normally distributed dataset the 95% would be at 2 standard deviations.\n",
    "    \n",
    "    # Using Polyfit Library\n",
    "    df_freq_fit2 = np.polynomial.polynomial.polyfit(x_values, y_values, 1)\n",
    "    y_intercept_min_year2 = df_freq_fit2[1]*x_values.min()+df_freq_fit2[0]\n",
    "\n",
    "    # COUNT TREND PLOT and PREDICTIVE PLOT\n",
    "    display(HTML(f'<h3>Trend and Prediction Plot<h3>'))\n",
    "    fig_width = min(int(len(x_values.unique()))+8, 14)\n",
    "    fig_width_ratio = min(4, int(len(x_values.unique())))\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, \n",
    "                                   gridspec_kw={'width_ratios': [fig_width_ratio, 1]},\n",
    "                                   sharey = True,\n",
    "                                   figsize=(fig_width, 8))\n",
    "\n",
    "    # FIRST PLOT: Plot of count time period data\n",
    "    ax1.bar(x = x_values, height = y_values)\n",
    "    ax1.set_title('Previous time periods count data\\n', size = 16)\n",
    "    ax1.set_xlabel(x_feature, fontsize=16)\n",
    "    ax1.set_ylabel(\"No. of Records\", fontsize=16)\n",
    "    ax1.set_ylim(bottom = 0, top = max(max(y_values)+max(y_values)*0.10, max(df_CDF.iloc[:,0]))) # Adjust the lower ylimit to 0 and upper to 10% of the max value.\n",
    "    ax1.axhline(lmbda, color=\"orange\", linestyle=\"dashed\") # Horizontal Line at the value of lambda Poisson parameter\n",
    "    \n",
    "    # Drawing the linear trendline\n",
    "    ax1.plot(x_values, linreg.coef_[0]*x_values+linreg.intercept_, color='red', linewidth = 2)\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # Show the values of each bar\n",
    "    for i in range(len(input_data)):\n",
    "        # Prints values on plot barchart\n",
    "        ax1.text(x_values[i], \n",
    "                 y_values[i]+y_values.max()*0.01, \n",
    "                 int_number_format(y_values[i], 0), \n",
    "                 size = 14)\n",
    "\n",
    "    # Secondary Plot: Predictions and Current Time Period\n",
    "    # Value of x needs to be 1 as the boxplot uses an index of 1. \n",
    "    ax2.set_title('Poisson Box Plot \\nLinear Trend Forecast (red bar)\\nCurrent Period Data (yellow bar)', size = 16)\n",
    "\n",
    "    # Post time period count data.\n",
    "    if timeperiod_predicted in df_data_counts[x_feature].tolist():\n",
    "        post_timeperiod_counts = df_data_counts['counts'].tolist()[df_data_counts[x_feature].tolist().index(timeperiod_predicted)] # If the timeperiod_predicted is found in the source data uses that year as the data to show.\n",
    "    else:\n",
    "        post_timeperiod_counts = 0\n",
    "    \n",
    "    ax2.bar(x = 1, height = post_timeperiod_counts, color = 'yellow', width = 0.6)\n",
    "    ax2.text(x = 1.3, y = post_timeperiod_counts*0.99, s = f'{post_timeperiod_counts}', size = 14)\n",
    "\n",
    "    # Drwaing BoxPlot Chart on Secondary Plot\n",
    "    ax2.boxplot(x = df_CDF[str(timeperiod_predicted)], patch_artist = True, labels = [df_CDF.columns[0]], widths = 0.2)\n",
    "\n",
    "    # Predicted value using the Linear Trend method \n",
    "    pred_value_LR = int(linreg.coef_[0]*((input_data.at[input_data.shape[0]-1, x_feature]+1)-(input_data.at[0, x_feature]))+y_intercept_min_year)\n",
    "    ax2.bar(x = 1, # Need to use this as the boxplot uses an index of 1 for the boxplot. \n",
    "            height = pred_value_LR, \n",
    "            color = 'red',\n",
    "            width = 0.25) \n",
    "    ax2.text(x = 0.95, y = pred_value_LR/2, s = f'{pred_value_LR}', size = 14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "\n",
    "    # PREDICTIVE STATISTICS SUMMARY TABLE\n",
    "    display(HTML(f'<h3>Summary<h3>'))\n",
    "    df_predictive_statistics = pd.DataFrame({'Description': ['Time Period Forecast',\n",
    "                                                             'Linear trendline equation',\n",
    "                                                             'Mean squared error',\n",
    "                                                             'Coefficient of Determination (R2)',\n",
    "                                                             'P-Value',\n",
    "                                                             'F-Statistic',\n",
    "                                                             'Standard Error',\n",
    "                                                             'Confidence Interval at 2 Std. Deviations',\n",
    "                                                             'Mean',\n",
    "                                                             'Variance',\n",
    "                                                             'Standard Deviation',\n",
    "                                                             'Linear trendline equation (Original)',\n",
    "                                                             'Forecast Value using the linear trendline',\n",
    "                                                             'Poisson: Figure Box Plot',\n",
    "                                                             'Poisson: Lambda Parameter',\n",
    "                                                             'Poisson: Probability at Lambda',\n",
    "                                                             'Poisson: 5th percentile',\n",
    "                                                             'Poisson: 25th percentile', \n",
    "                                                             'Poisson: 50th percentile',\n",
    "                                                             'Poisson: 75th percentile',\n",
    "                                                             'Poisson: 95th percentile',\n",
    "                                                             'Poisson: Kruskal-Wallis H statistic, corrected for ties',\n",
    "                                                             'Poisson: Kruskal-Wallis P-value'],\n",
    "                                             'Value': [str(timeperiod_predicted),\n",
    "                                                       str('y={:.2f}*x+{:.2f}'.format(linreg.coef_[0].astype(float), y_intercept_min_year)),\n",
    "                                                       round(mean_squared_error(y_values, y_pred), 2),\n",
    "                                                       round(r2_score(y_values, y_pred), 2),\n",
    "                                                       round(p_value[0], 6),\n",
    "                                                       round(f_statistic[0], 4),\n",
    "                                                       round(std_err_scipy, 4),\n",
    "                                                       round(confidence_interval_scipy, 4),\n",
    "                                                       round(y_values.values.mean()),\n",
    "                                                       round(y_values.values.var()),\n",
    "                                                       round(y_values.values.std()),\n",
    "                                                       str('y={:.2f}*x+{:.2f}'.format(df_freq_fit2[1], y_intercept_min_year2)),\n",
    "                                                       pred_value_LR,\n",
    "                                                       str('N/A'),\n",
    "                                                       str(lmbda),\n",
    "                                                       str(round(poisson.pmf(k=lmbda, mu=lmbda),4)),\n",
    "                                                       str(int(np.interp(0.05, df_CDF['probability'], df_CDF[str(timeperiod_predicted)]))), \n",
    "                                                       str(int(np.interp(0.25, df_CDF['probability'], df_CDF[str(timeperiod_predicted)]))), \n",
    "                                                       str(int(np.interp(0.50, df_CDF['probability'], df_CDF[str(timeperiod_predicted)]))), \n",
    "                                                       str(int(np.interp(0.75, df_CDF['probability'], df_CDF[str(timeperiod_predicted)]))), \n",
    "                                                       str(int(np.interp(0.95, df_CDF['probability'], df_CDF[str(timeperiod_predicted)]))),\n",
    "                                                       stats.kruskal(x_values.values.tolist(), y_values.values.tolist())[0], #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html\n",
    "                                                       stats.kruskal(x_values.values.tolist(), y_values.values.tolist())[1]],\n",
    "                                             'Notes': ['Time period used for forecasting (e.g., CY, calendar quarter, FY, fiscal quarter, etc.).',\n",
    "                                                       '\"y = mx+b\" where: \"m\" is trend slope, \"x\" is Max-Min time periods and \"b\" is intercept at Min time period.',\n",
    "                                                       'MEAN SQUARE ERROR ADD EXPLANATION NOTE',\n",
    "                                                       'R2 SCORE EXPLANATION NOTE: CONFIRMED WITH STATISTICIAN VALUE.',\n",
    "                                                       'P-VALUE EXPLANATION NOTE: CONFIRMED WITH STATISTICIAN VALUE.',\n",
    "                                                       'F-STATISTIC EPXLANATION NOTE: CONFIRMED WITH STATISTICIAN VALUE.',\n",
    "                                                       'STANDARD ERROR: ADD EXPLANATION NOTE.',\n",
    "                                                       'CONFIDENCE INTERVAL: ADD EXPLANATION NOTE',\n",
    "                                                       'Mean of the Values',\n",
    "                                                       'Variance of the Values',\n",
    "                                                       'Standard Deviation of the Values',\n",
    "                                                       'Linear trendline equation using Polyfit Library',\n",
    "                                                       'Forecast value at the specified time period above.',\n",
    "                                                       'For the definition of boxplot quartiles see: https://en.wikipedia.org/wiki/Box_plot.',\n",
    "                                                       'Average rate at which records are being observed/specified/calculated (i.e., dashed orange line in plot).', \n",
    "                                                       'Probability that there are exactly lambda records in the predicted time period.', \n",
    "                                                       'There is a 0.05 probability that the there will be less than the specified number of records.', \n",
    "                                                       'There is a 0.25 probability that the there will be less than the specified number of records.', \n",
    "                                                       'There is a 0.50 probability that the there will be less than the specified number of records.', \n",
    "                                                       'There is a 0.75 probability that the there will be less than the specified number of records.', \n",
    "                                                       'There is a 0.95 probability that the there will be less than the specified number of records.',\n",
    "                                                       \"NEED VALUE CONFIRMATION: Poisson's distribution Kruskal-Wallis H statistic, corrected for ties\",\n",
    "                                                       \"NEED VALUE CONFIRMATION: Poisson's distribution Kruskal-Wallis P-value for the test using the assumption that H has a chi square distribution. The p-value returned is the survival function of the chi square distribution evaluated at H. The chi-square approximation may not be accurate with low number of samples.\"]\n",
    "                                            })\n",
    "\n",
    "    display(HTML(df_predictive_statistics.to_html(index=False)))\n",
    "\n",
    "    # POISSON PDF and CDF Plots\n",
    "    display(HTML(f'<h3>Poisson PDF and CDF<h3>'))\n",
    "    fig, ax1 = plt.subplots() # Figure with two axes\n",
    "    # Plot for the CDF\n",
    "    ax1.set_title(f'Poisson PDF and CDF with sample size of {sample_size}', size = 16)\n",
    "    ax1.set_xlabel('# of Records', size = 14)\n",
    "    ax1.set_ylabel('Cumulative Probability', size = 14, color = 'blue')\n",
    "    ax1.plot(x_random_sorted, cdf_y_values, color = 'blue')\n",
    "    ax1.tick_params(axis='y', size = 14)\n",
    "\n",
    "    # Plot of Poisson distribution using the random sample size of \"sample_size\".\n",
    "    ax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis\n",
    "    ax2.set_ylabel('# of Samples', color = 'red', size = 14)  # we already handled the x-label with ax1\n",
    "    ax2.hist(x_random, histtype = u'step', color = 'red')\n",
    "    ax2.tick_params(axis='y', size = 14)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show();\n",
    "    \n",
    "    # SCATTER PLOT WITH CONFIDENCE INTERVALS AT 95% https://stackoverflow.com/questions/27164114/show-confidence-limits-and-prediction-limits-in-scatter-plot\n",
    "    p, cov = np.polyfit(x_values, y_values, 1, cov=True) # parameters and covariance from of the fit of 1-D polynom.\n",
    "\n",
    "    # Statistics\n",
    "    n = y_values.size # number of observations\n",
    "    m = p.size # number of parameters\n",
    "    dof = n - m # degrees of freedom\n",
    "    confidence_interval = 0.95 # Confidence interval\n",
    "    t = stats.t.ppf(confidence_interval, n - m) # used for CI and PI bands\n",
    "    # Estimates of Error in Data/Model\n",
    "    resid = y_values - y_pred                          \n",
    "    chi2 = np.sum((resid / y_pred)**2) # chi-squared; estimates error in data\n",
    "    chi2_red = chi2 / dof # reduced chi-squared; measures goodness of fit\n",
    "    s_err = np.sqrt(np.sum(resid**2) / dof) # standard deviation of the error\n",
    "\n",
    "    # Plotting --------------------------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.set_title(f'Record Counts vs. {x_feature} with {confidence_interval} confidence interval', size = 16)\n",
    "    ax.set_ylabel('Record Counts', size = 14)\n",
    "    ax.set_xlabel(f'{x_feature}', size = 14)\n",
    "    \n",
    "    # Data\n",
    "    ax.plot(x_values, y_values, \"o\", color=\"#b9cfe7\", markersize=8, \n",
    "        markeredgewidth=1, markeredgecolor=\"b\", markerfacecolor=\"None\")\n",
    "\n",
    "    # Fit\n",
    "    ax.plot(x_values, y_pred, \"-\", color=\"0.1\", linewidth=1.5, alpha=0.5, label=\"Fit\")  \n",
    "    \n",
    "    x2 = np.linspace(np.min(x_values), np.max(x_values), 100)\n",
    "    y2 = np.polyval(p, x2)\n",
    "\n",
    "    # Confidence Interval (select one)\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    ci_test = t * s_err * np.sqrt(1/n + (x2 - np.mean(x_values))**2 / np.sum((x_values - np.mean(x_values))**2))\n",
    "    ax.fill_between(x2, y2 + ci_test, y2 - ci_test, color=\"#b9cfe7\")\n",
    "   \n",
    "    # Prediction Interval\n",
    "    pi = t * s_err * np.sqrt(1 + 1/n + (x2 - np.mean(x_values))**2 / np.sum((x_values - np.mean(x_values))**2))   \n",
    "    ax.fill_between(x2, y2 + pi, y2 - pi, color=\"None\", linestyle=\"--\")\n",
    "    ax.plot(x2, y2 - pi, \"--\", color=\"0.5\", label=\"95% Prediction Limits\")\n",
    "    ax.plot(x2, y2 + pi, \"--\", color=\"0.5\")\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reports Widget Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "The Reports widget functions give the ability to export the df_data_filtered. The user can specify which columns to export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_reports_widget():\n",
    "    interact_manual(filtered_reports_options_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_reports_options_widget():\n",
    "    interact_manual(filtered_reports,\n",
    "                    columns_to_show_W = widgets.SelectMultiple(options=df_data_filtered.columns,\n",
    "                                                               value = ['original_title', 'overview', 'budget', \n",
    "                                                                        'revenue', 'runtime'],\n",
    "                                                               rows = 8,\n",
    "                                                               description = 'Data Features to Show', \n",
    "                                                               disabled=False, style={'description_width': 'initial'}),\n",
    "                    row_range_W = widgets.IntRangeSlider(value=[0, 20],\n",
    "                                                         min=0, \n",
    "                                                         max=df_data_filtered.shape[0],\n",
    "                                                         step=20,\n",
    "                                                         description='Row Selection:', disabled=False,\n",
    "                                                         continuous_update=False,\n",
    "                                                         orientation='horizontal',\n",
    "                                                         readout=True,\n",
    "                                                         readout_format='d',\n",
    "                                                         style = {'description_width': 'initial'},\n",
    "                                                         layout = widgets.Layout(width='40%')),\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_reports(columns_to_show_W, row_range_W):\n",
    "    # Styling. This changes the dataframe to a Styler Object.\n",
    "    columns_to_hide = list(set(columns_to_show_W).symmetric_difference(set(df_data_filtered.columns)))\n",
    "    \n",
    "    styled_df_filtered = df_data_filtered.loc[row_range_W[0]:row_range_W[1]].copy() # Keeps the original dataframe.\n",
    "    # Otherwise the function below changes the format of the df_data_filtered after this function runs.\n",
    "    \n",
    "    # NOTE FUTURE WARNING on style.set_properties() being depracated by styler.hide()\n",
    "    # However: Styler documentation still shows style function examples as of Dec 2022. \n",
    "    styled_df_filtered = styled_df_filtered.style.set_properties(subset=['original_title_overview', 'norm_text_stem', 'norm_text_lemma'], \n",
    "                                                                 **{'width': '400px'},\n",
    "                                                                ).hide_columns(columns_to_hide).hide_index()\n",
    "\n",
    "    return(styled_df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Filtered Dataframe Function\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_df_widget():\n",
    "    interact_manual(export_df_data_filtered, \n",
    "                    file_type_W = widgets.Dropdown(options=['CSV', 'Excel', 'JSON'],\n",
    "                                                            value = 'CSV',\n",
    "                                                            description = 'Export Filtered Data (File Type)', \n",
    "                                                            disabled=False, style={'description_width': 'initial'},\n",
    "                                                            layout=widgets.Layout(width='50%'))\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_df_data_filtered(file_type_W):\n",
    "    if file_type_W == 'CSV':\n",
    "        df_data_filtered.to_csv(r'.\\output_data\\df_data_filtered.csv', encoding='utf-8-sig', index = False, header = True)\n",
    "    if file_type_W == 'Excel':\n",
    "        df_data_filtered.to_excel(r'.\\output_data\\df_data_filtered.xlsx', encoding='utf-8-sig', index = False, header = True)\n",
    "    if file_type_W == 'JSON': # NEED TO TEST WITH THE FUNCTION.\n",
    "        print('NOT CONVERTED YET NEED TO ADD FUNCTION AND TEST')\n",
    "        #df_data_filtered.to_json(r'.\\output_data\\df_data_filtered.xlsx', orient= 'record', date_format=None, force_ascii=True, lines=False, index=True, indent=None, storage_options=None)\n",
    "    print(f'File successfully exported as {file_type_W}.')\n",
    "# Encoding \"cp1252\" or \"utf-8-sig\" used so that Excel does not create special characters. Standard Python is utf-8.\n",
    "# See reference for explanation https://stackoverflow.com/questions/57061645/why-is-%C3%82-printed-in-front-of-%C2%B1-when-code-is-run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reclustering: Filters the Selected Cluster\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "The reclustering widget allows the user to update the df_data_filtered with the records that meet specified cluster number. User can then go back to any of the tabs and use the functions with the filtered cluster data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recluster_widget():\n",
    "    interact_manual(recluster_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recluster_options():\n",
    "    global CLUSTER_LABEL_list\n",
    "    if 'cluster_label_rev' in list(df_data_filtered.columns): # Checks if Clustering has been performed by checking the existence of 'cluster_label_rev' column.\n",
    "        CLUSTER_LABEL_list = (df_data_filtered['cluster_label_rev'].unique()).tolist()\n",
    "        CLUSTER_LABEL_list.sort()\n",
    "        CLUSTER_LABEL_list = list(dict.fromkeys([element for element in CLUSTER_LABEL_list])) # Removes Duplicate\n",
    "        recluster_select_cluster_widget(cluster_options = CLUSTER_LABEL_list)\n",
    "    elif 'cluster_label_rev' not in list(df_data_filtered.columns): # If clustering has not been performed print message.\n",
    "        print('Please run the \"Text Analysis and Modeling\" module with either Kmeans or DBSCAN.')\n",
    "    elif len(df_data_filtered['cluster_label_rev'].unique()) == 1: # Checks if the Clustering has been performed.\n",
    "        print('Please run the \"Text Analysis and Modeling\" module with either Kmeans or DBSCAN.')\n",
    "    else:\n",
    "        print('Please run the \"Text Analysis and Modeling\" module with either Kmeans or DBSCAN.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recluster_select_cluster_widget(cluster_options):\n",
    "    interact_manual(recluster_select_cluster,\n",
    "                    selected_cluster = widgets.Dropdown(options = ['None']+CLUSTER_LABEL_list,\n",
    "                                                    value = 'None',\n",
    "                                                    description = 'Cluster To Filter', \n",
    "                                                    disabled=False, style={'description_width': 'initial'},\n",
    "                                                    layout=widgets.Layout(width='40%'))\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recluster_select_cluster(selected_cluster):\n",
    "    global df_data_filtered, reclustering_counter\n",
    "    if selected_cluster != 'None':\n",
    "        df_data_filtered = df_data_filtered.loc[(df_data_filtered['cluster_label_rev'].isin(list(str(selected_cluster))))\n",
    "                                               ].reset_index(drop = True)\n",
    "        reclustering_counter = reclustering_counter+1\n",
    "        df_unique_values_dict()\n",
    "        print(f'Filtering of cluster {selected_cluster} complete.')\n",
    "    else:\n",
    "        print(f'Please Select a Cluster Number.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Filter Widget Test Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Data Filtering Widget\n",
    "[Return to Table of Contents](#Table-of-Contents)<br>\n",
    "\n",
    "References: <br>\n",
    "- List of widgets:https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20List.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>MAIN FILTER WIDGET<h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Caution when selecting filters. Some filters may have overlaps (e.g., FY and CY) while other filters be mutually exclusive.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41a6db89da744f88962cde662f958d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntRangeSlider(value=(1916, 2016), continuous_update=False, description='Release Calenda…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PRIMARY WIDGET FILTER OPTIONS\n",
    "display(HTML(f'<h3>MAIN FILTER WIDGET<h3>'))\n",
    "print(\"NOTE: Caution when selecting filters. Some filters may have overlaps (e.g., FY and CY) while other filters be mutually exclusive.\")\n",
    "\n",
    "_ = interact_manual(filter_data, # \"filter_data\" function filters the data and calls sub-widgest and related functions.\n",
    "                    release_cy_filter = widgets.IntRangeSlider(value=[min(release_cy_list), max(release_cy_list)],\n",
    "                                                            min=min(release_cy_list),\n",
    "                                                            max=max(release_cy_list),\n",
    "                                                            step=1,\n",
    "                                                            description='Release Calendar Year', disabled=False,\n",
    "                                                            continuous_update=False,\n",
    "                                                            orientation='horizontal',\n",
    "                                                            readout=True,\n",
    "                                                            readout_format='d',\n",
    "                                                            style = {'description_width': 'initial'},\n",
    "                                                            layout = widgets.Layout(width='50%')),\n",
    "                    mult_genres_filter = widgets.RadioButtons(options=['OR', 'AND'],\n",
    "                                                              value='OR', # Default\n",
    "                                                              description='Multiple Genres Selection',\n",
    "                                                              disabled=False, style={'description_width': 'initial'}),\n",
    "                    genres_filter = widgets.SelectMultiple(options= genres_list,\n",
    "                                                             value = genres_list,\n",
    "                                                             rows = 6,\n",
    "                                                             description = 'Genres', \n",
    "                                                             disabled=False, style={'description_width': 'initial'}),\n",
    "                    SIM_SEARCH_filter = widgets.Text(value='',\n",
    "                                                     placeholder='Type something to search',\n",
    "                                                     description='Similarity Search',\n",
    "                                                     disabled=False, \n",
    "                                                     style={'description_width': 'initial'}, \n",
    "                                                     layout = widgets.Layout(width='50%')),\n",
    "                    SIM_SEARCH_THRESHOLD_filter = widgets.FloatSlider(min=0.00, \n",
    "                                                                      max=1.0, \n",
    "                                                                      value=0.00, \n",
    "                                                                      step=0.01,\n",
    "                                                                      description=\"Similarity Search Threshold\",\n",
    "                                                                      disables=False, \n",
    "                                                                      style={'description_width': 'initial'},\n",
    "                                                                      layout=widgets.Layout(width='50%'),\n",
    "                                                                      readout_format='.2f'),  \n",
    "                    SIM_SEARCH_WORD_PROCESS_filter = widgets.Dropdown(options= ['Lemmatization', 'Stemming'], \n",
    "                                                                      description = 'Similarity Search Word Preprocessing',\n",
    "                                                                      value = 'Stemming',\n",
    "                                                                      disabled=False, style={'description_width': 'initial'},\n",
    "                                                                      layout=widgets.Layout(width='40%')),\n",
    "                    SIM_SEARCH_VECT_filter = widgets.Dropdown(options= ['TFIDF', 'Count'], \n",
    "                                                              description = 'Similarity Search Word Vectorizer',\n",
    "                                                              value = 'TFIDF',\n",
    "                                                              disabled=False, style={'description_width': 'initial'}),\n",
    "                    STOPWORDS_filter = widgets.Text(value='',\n",
    "                                                    placeholder='Add stopwords followed by comma.',\n",
    "                                                    description='Add Stopwords',\n",
    "                                                    disabled=False, \n",
    "                                                    style={'description_width': 'initial'},\n",
    "                                                    layout = widgets.Layout(width='50%')),\n",
    "                    PRINT_filter=widgets.Dropdown(options= ['NO', 'YES'], \n",
    "                                                  description = 'Print Report',\n",
    "                                                  value = 'NO',\n",
    "                                                  disabled=False, style={'description_width': 'initial'})\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Filter to Columns\n",
    "budget <br>\n",
    "original_language<br>\n",
    "popularity<br>\n",
    "production_companies (categorical)<br>\n",
    "production_countries (Categorical)<br>\n",
    "vote_average<br>\n",
    "vote_count<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIX\n",
    "C:\\Users\\felix\\AppData\\Local\\Temp\\ipykernel_6432\\2648881204.py:6: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='columns')`\n",
    "  styled_df_filtered = styled_df_filtered.style.set_properties(subset=['original_title_overview', 'norm_text_stem', 'norm_text_lemma'],\n",
    "C:\\Users\\felix\\AppData\\Local\\Temp\\ipykernel_6432\\2648881204.py:6: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
    "  styled_df_filtered = styled_df_filtered.style.set_properties(subset=['original_title_overview', 'norm_text_stem', 'norm_text_lemma'],\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
